[{"content":" A deadlock happens when there\u0026rsquo;s at least one resource that can be only acquired by one process at a time and there\u0026rsquo;s a process P1 that is waiting to acquire a resource currently held by a process P2 and P2 is waiting to acquire a resource currently held by P1.\nThe word process does not mean we are talking about OS processes.\nThere are two processes: Process A wants to acquire a lock on resource 1 and then a lock on resource 2. Process B wants to acquire a lock on resource 2 and then a lock on resource 1.\nIn this specific execution process A acquires a lock on resource 1 and process B acquires a lock on resource 2, when process A tries to acquire a lock on resource 2 it blocks waiting for process B to release the lock that it is holding on resource 2. The problem is that the lock on resource 2 will never be released because process B is waiting for process A to release the lock on resource 1.\nDatabases are being used as an example but deadlocks and the solutions do not appear only in databases.\nProcess A Process B LOCK 1 LOCK 2 LOCK 2 LOCK 1 RELEASE 1 RELEASE 2 RELEASE 2 RELEASE 1 A table showing the operations performed by each process concurrently. Deadlock occurs at the second set of operations when process A tries to lock resource 2 and process B tries to lock resource 1.\nDeadlocks can be easily avoided in this case by having both processes try to acquire locks on resources in the same order.\nProcess A Process B LOCK 1 LOCK 1 LOCK 2 LOCK 2 RELEASE 1 RELEASE 1 RELEASE 2 RELEASE 2 By having both processes try to acquire the locks in the same order, the process that acquires the first lock will be able to proceed and acquire the other lock while the second process will block waiting for the first lock instead of acquiring another lock. When the first lock is released the process that is waiting for it will be able to proceed.\nProcess A acquires a lock on resource 1, process B tries to acquire the same lock but blocks because the resource is already locked, process A proceeds and acquires a lock on resource 2 while process B is still blocked waiting for the lock on resource 1. Process B will be able to acquire locks on resource 1 and resource 2 only after process A releases them.\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/the_simple_way_to_avoid_deadlocks/","summary":"A deadlock happens when there\u0026rsquo;s at least one resource that can be only acquired by one process at a time and there\u0026rsquo;s a process P1 that is waiting to acquire a resource currently held by a process P2 and P2 is waiting to acquire a resource currently held by P1.\nThe word process does not mean we are talking about OS processes.\nThere are two processes: Process A wants to acquire a lock on resource 1 and then a lock on resource 2.","title":"The simple way to avoid deadlocks"},{"content":" Anomalies An anomaly or read phenomena can happen when a transaction reads data that may have been modified by another concurrent transaction.\nDirty read A dirty read happens when a transaction T1 reads data that has been modified by a concurrent transaction T2 that has not has been committed or rolled back yet. T1 ends up working with stale data if T2 does not commit.\nT2 starts executing and sets x to a new value, T1 starts executing and reads x, the value of x is the value just set by T2, T2 rolls back, the value of x is not persisted to the database but T1 will move forward with the stale value of x that was written before T2 rolled back.\nNon-repeatable read A non-repeatable read happens when a transaction T1 reads a value x before and after it is modified by a transaction T2 and T2 has committed. If x has been assigned a new value by T2, T1 will have seen two different values for the same variable in a transaction.\nT1 reads x with value 0, T2 updates x to 1 and commits and then T1 reads x again but the value is now 1.\nPhantom reads A phantom read is almost like a non-repeatable read but it is said to happen when more than one row is being selected.\nA transaction T1 executes a statement to select a set of rows, a concurrent transaction T2 executes a statement that modifies, inserts or deletes a row and commits. T1 executes the first statement again hoping to select the same set of rows but a different set is returned.\nT1 starts executing and selects a set of rows from a, while T1 executing, T2 inserts a new row into a and commits, when T1 tries to read the same set of rows from a it gets a different set because a new element was added by T2.\nLost updates A lost update may happen when transactions T1 and T2 both read and try to update x using the just read value. Similarly to what happens when two threads try to update a variable without synchronization, one of the transactions may read x just a little bit before the other one updates it and end up working with a stale value.\nIn this example, there are two transactions T1and T2. Both are executing concurrently and both read x and increment its value by 1, if x had the value 0 the expectation would be that the new value of x would be 2 after both transactions commit but the reality is that the new value of x will be 1 because both transactions read 0 before incrementing it by 1.\nDirty writes A dirty write happens when a transaction reads an uncommitted value from another transaction, modifies and writes it.\nT1 and T2 are executing concurrently, T2 sets x to 1, T1 reads x with value 1, T2 rolls back, T1 increments x by 1 resulting in 2 and writes it. The value of x is 2 instead of its previous value increased by 1.\nWrite skew A write skew happens when concurrent transactions respect database invariants(e.g table.x \u0026gt; 0) but when committed break one or more invariants.\nAssuming that a.balance + b.balance \u0026gt; 0 is an invariant, both transactions T1 and T2 respect the invariant on their own and are allowed to commit, but after both transactions are committed the invariant is broken.\nIsolation levels Read and write anomalies can be avoided by choosing the right isolation level.\nRead uncomitted Dirty reads are allowed, transactions can read data that has been modified by other transactions even if they have not been committed yet.\nRead committed Allows transaction T1 to read data that has been committed by other transactions while T1 is still executing, repeatable reads are not guaranteed.\nRepeatable reads Same as read committed but reading x will always result in the same value.\nSerializable Each transaction behaves like they are executed to completion before other transactions starts executing.\nReferences Database internals\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/isolation_levels/","summary":"Anomalies An anomaly or read phenomena can happen when a transaction reads data that may have been modified by another concurrent transaction.\nDirty read A dirty read happens when a transaction T1 reads data that has been modified by a concurrent transaction T2 that has not has been committed or rolled back yet. T1 ends up working with stale data if T2 does not commit.\nT2 starts executing and sets x to a new value, T1 starts executing and reads x, the value of x is the value just set by T2, T2 rolls back, the value of x is not persisted to the database but T1 will move forward with the stale value of x that was written before T2 rolled back.","title":"Database anomalies and isolation levels"},{"content":"The problem You are developing an application backed by a database, something happens and then several of your users try to access the same content. Several requests are sent to your backend at almost the same time and your backend hits the database once for each request to fetch the same data.\nFetching the data only once If N requests asking for the same data arrive at the backend at around the same time, the backend could hit the database to fetch the data when the first request arrives and force the other requests to await until the data is fetched. When a response to the request sent to the database arrives at the backend with the data, the data can be shared with the requests that are waiting for it.\nSeveral servers If there\u0026rsquo;s more than one server it is possible that the number of requests sent to the database is equal to the number of servers because requests are load balanced between the servers available.\nIf there\u0026rsquo;s a large number of servers, it may be desirable to route requests for the same data to the same server to decrease the number of requests sent to the database. In this case every request to GET X is routed to the first server.\nExample Given a route to fetch some data by its id, the id can be used to decide when a request can be deduplicated.\nasync fn handler(id: u64) -\u0026gt; Response { let data = deduplicator.dedup(id, fetch(id)).await; Response::new(data) } Example deduplicator implementation\nNotes This post was inspired by Discord\u0026rsquo;s How Discord stores trillions of messages\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/request_coalescing/","summary":"The problem You are developing an application backed by a database, something happens and then several of your users try to access the same content. Several requests are sent to your backend at almost the same time and your backend hits the database once for each request to fetch the same data.\nFetching the data only once If N requests asking for the same data arrive at the backend at around the same time, the backend could hit the database to fetch the data when the first request arrives and force the other requests to await until the data is fetched.","title":"Avoid overloading your systems: Request coalescing"},{"content":"Uber has adopted Go as its primary programming language for developing microservices and has a post on its blog called Data Race Patterns in Go where they talk about data races found in their Go codebase.\nI was reading it and thought to myself that many of the problems presented in the post would not even compile in Rust. Can Rust help us avoid writing code with common data races?\nExamples written in Rust are not meant do be idiomatic Rust and do not wait for outputs generated by tasks for simplicity because the examples written in Go do not wait as well.\nWhat is a data race A data race happens when a task tries to access memory while another task tries to write to it at the same time. The task that\u0026rsquo;s accessing memory may read a value that was just modified, is being modified or will be modified and end up processing an unexpected value.\nLoop index variable capture Go has the range loop to iterate over a collection.\nfor index, value := range list { ... } index is the index of the element in the list and value is the value at that index. The problem with it is that value is not a new variable at each iteration, it is just modified to have the value of the current iteration. This behaviour in combination with the fact that closures capture variables by reference instead of by copy can easily lead to concurrency bugs.\nWhile ProcessJob is running, the range loop will be updating the job variable which was captured by reference by ProcessJob, it is possible that by that ProcessJob gets to run, job will be a reference to the last job in jobs because the variable has been updated.\nfor _, job := range jobs { go func(){ ProcessJob(job) }() } There\u0026rsquo;s a proposal to change for range semantics in Go.\nRust provides concurrency through stackless coroutines, also known as async and await, the only thing we need to do is to choose a async runtime. I chose to use tokio which is the most popular runtime in the Rust ecosystem at the moment.\nWe can iterate through any collection that implements the Iterator trait by using the for in construct.\nfor job in jobs { tokio::spawn(process_job(job)); } for job in jobs is syntax sugar for for job in jobs.into_iter() \u0026ndash; it also is syntax sugar for advancing an iterator, but we don\u0026rsquo;t care about that \u0026ndash; which lets us iterate over the values in a collection by consuming the collection.\nWe could iterate over references to the values inside of the collection by using for job in jobs.iter() which creates an iterator that yields a reference to each value of the collection, if needed but the code would not compile in this case because a task(think goroutine) may run for longer than the amount of time the value being referenced by it lives for.\nerror[E0597]: `jobs` does not live long enough --\u0026gt; src/for_range_and_closures.rs:6:16 | 6 | for job in jobs.iter() { | ^^^^^^^^^^^ borrowed value does not live long enough 7 | tokio::spawn(process_job(job)); | ---------------- argument requires that `jobs` is borrowed for `\u0026#39;static` 8 | } 9 | } | - `jobs` dropped here while still borrowed There\u0026rsquo;s no need to use a closure but for completeness sake here it is and it would not compile as well.\nerror[E0597]: `jobs` does not live long enough --\u0026gt; src/for_range_and_closures.rs:14:16 | 14 | for job in jobs.iter() { | ^^^^^^^^^^^ borrowed value does not live long enough 15 | tokio::spawn((|| async { process_job(job) })()); | -------------------------- returning this value requires that `jobs` is borrowed for `\u0026#39;static` 16 | } 17 | } | - `jobs` dropped here while still borrowed Data race due to idiomatic err variable capture Errors are values in Go and failable functions usually return a tuple with two values, one value being the result if the function succeeded and the other being an error if the function failed.\nIn this example, processing is happening concurrently between the current function and a goroutine. We want to return an error if the goroutine or the current function fails, so they both try to assign to the err variable so the error can be returned to the caller of the current function.\nx, err := Foo() if err != nil { ... } go func() { var y int // Oops, err was captured by reference. y, err = Bar() if err != nil { ... } }() var z int // err is being written to by the goroutine as well. z, err = Baz() if err != nil { ... } This is an example of a data race because the err variable is being modified by the current function and by the goroutine without synchronization.\nAs expected, the same example does not compile in Rust for several reasons.\nerror[E0373]: async block may outlive the current function, but it borrows `result`, which is owned by the current function --\u0026gt; src/goroutine_var_reference_data_race.rs:41:24 | 41 | tokio::spawn(async { | ________________________^ 42 | | // Trying to assign to the `result` captured . 43 | | result = bar(); | | ------ `result` is borrowed here 44 | | if result.is_err() { 45 | | // ... 46 | | } 47 | | }); | |_____^ may outlive borrowed value `result` | = note: async blocks are not executed immediately and must either take a reference or ownership of outside variables they use help: to force the async block to take ownership of `result` (and any other referenced variables), use the `move` keyword | 41 | tokio::spawn(async move { | ++++ error[E0506]: cannot assign to `result` because it is borrowed --\u0026gt; src/goroutine_var_reference_data_race.rs:51:5 | 41 | tokio::spawn(async { | _____-__________________- | |_____| | || 42 | || // Trying to assign to the `result` captured . 43 | || result = bar(); | || ------ borrow occurs due to use in generator 44 | || if result.is_err() { 45 | || // ... 46 | || } 47 | || }); | ||_____-- argument requires that `result` is borrowed for `\u0026#39;static` | |_____| | borrow of `result` occurs here ... 51 | result = baz(); | ^^^^^^ assignment to borrowed `result` occurs here error[E0502]: cannot borrow `result` as immutable because it is also borrowed as mutable --\u0026gt; src/goroutine_var_reference_data_race.rs:52:8 | 41 | tokio::spawn(async { | _____-__________________- | |_____| | || 42 | || // Trying to assign to the `result` captured . 43 | || result = bar(); | || ------ first borrow occurs due to use of `result` in generator 44 | || if result.is_err() { 45 | || // ... 46 | || } 47 | || }); | ||_____-- argument requires that `result` is borrowed for `\u0026#39;static` | |_____| | mutable borrow occurs here ... 52 | if result.is_err() { | ^^^^^^^^^^^^^^^ immutable borrow occurs here We could try to translate the Go version to Rust and make it work by using atomic reference counting with Arc and a Mutex.\nlet result = foo(); if result.is_err() { // ... } let result = Arc::new(Mutex::new(result)); let result_clone = Arc::clone(\u0026amp;result); let task = tokio::spawn(async move { let mut result = result_clone.lock().await; *result = bar(); if result.is_err() { // ... } }); { let mut result = result.lock().await; *result = baz(); if result.is_err() { // ... } } task.await; Arc::try_unwrap(result).unwrap().into_inner() Bugs due to copies of a slice\u0026rsquo;s internal state Go slices are growable lists made of a pointer to a buffer, a capacity and a length. We can add elements to a slice by calling append, if the slice is full, it will grow to accommodate the new element.\nThe problem here is that by passing myResults as argument to the closure, we are copying the length, the capacity and the pointer to the buffer of the slice because function arguments are passed by copy in Go.\nWhen we try to append to myResults, the copy the goroutine holds may have the wrong length and capacity because myResults may have need to grow when another goroutine appended to it.\nfunc ProcessAll(uuids []string) { var myResults []string var mutex sync.Mutex safeAppend := func(res string) { mutex.Lock() myResults = append(myResults, res) mutex.Unlock() } for _, uuid := range uuids { go func(id string, results []string) { res := Foo(id) safeAppend(res) }(uuid, myResults) } } Like Go, Rust function arguments are passed by copy as well but unlike Go, Rust\u0026rsquo;s Vec, the growable array, is not passed by reference by default.\nIf we try to translate the Go code to Rust, it does not even compile. We create a closure safe_append to ensure the mutex is always locked before modyfing the list of results, it does not compile because we try to use the closure inside several tasks but the closure gets moved after the first loop iteration.\nfn process_all(uuids: Vec\u0026lt;String\u0026gt;) { let mut my_results = Vec::new(); let mutex = Mutex::new(()); let mut safe_append = |res: String| async move { mutex.lock().await; my_results.push(res); }; for uuid in uuids { tokio::spawn(async { let res = foo(uuid); safe_append(res); }); } } error[E0382]: use of moved value: `safe_append` --\u0026gt; src/mutex_slice_append.rs:34:28 | 34 | tokio::spawn(async { | ____________________________^ 35 | | let res = foo(uuid); 36 | | safe_append(res); | | ----------- use occurs due to use in generator 37 | | }); | |_________^ value moved here, in previous iteration of loop The solution is simple as well. Rust\u0026rsquo;s Mutex is meant to hold the data being protected instead of just acting like a type of flag. Use reference counting so each task can access the same mutex and modify the data held by it.\nfn process_all_2(uuids: Vec\u0026lt;String\u0026gt;) { let my_results = Arc::new(Mutex::new(Vec::new())); for uuid in uuids { let my_results_clone = Arc::clone(\u0026amp;my_results); tokio::spawn(async move { let res = foo(uuid); let mut my_results = my_results_clone.lock().await; my_results.push(res); }); } } The mutex is unlocked automatically on Drop thanks to RAII.\nData races because maps are not thread-safe In this example, orders are being processed concurrently and eventual errors are added to a map where the key is the order id and the error is the value so we can know which orders were not processed.\nThe problem is that the map is not thread-safe which means that since there are several goroutines modifying the map without synchronization it may end up in an unexpected state.\nfunc processOrders(uuids []string) error { var errMap = make(map[string]error) for _, uuid := range uuids { go func(uuid string) { orderHandle, err := GetOrder(uuid) if err != nil { // Data race errMap[uuid] = err return } ... }(uuid) } return combineErrors(errMap) } As expected, the same code does not compile in Rust because multiple tasks(think goroutine) may not have mutable access to a value at the same time without synchronization.\nfn process_orders(uuids: Vec\u0026lt;String\u0026gt;) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut err_map = HashMap::new(); for uuid in uuids { tokio::spawn(async { match get_order(\u0026amp;uuid).await { Err(err) =\u0026gt; { err_map.insert(uuid, err); } Ok(value) =\u0026gt; { // ... } } }); } combine_errors(err_map) } error[E0499]: cannot borrow `err_map` as mutable more than once at a time --\u0026gt; src/thread_unsafe_hashmap.rs:23:28 | 23 | tokio::spawn(async { | _________-__________________^ | |_________| | || 24 | || match get_order(\u0026amp;uuid).await { 25 | || Err(err) =\u0026gt; { 26 | || err_map.insert(uuid, err); | || ------- borrows occur due to use of `err_map` in generator ... || 31 | || } 32 | || }); | ||_________^- argument requires that `err_map` is borrowed for `\u0026#39;static` | |_________| | `err_map` was mutably borrowed here in the previous iteration of the loop The correct version is also pretty simple in this case. Use a mutex to protect the data so it can be mutate by several tasks concurrently and reference counting to ensure the every task operates on the data guarded by the same mutex.\nfn process_orders_2(uuids: Vec\u0026lt;String\u0026gt;) -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn std::error::Error\u0026gt;\u0026gt; { let mut err_map = Arc::new(Mutex::new(HashMap::new())); for uuid in uuids { let err_map_clone = Arc::clone(\u0026amp;err_map); tokio::spawn(async move { match get_order(\u0026amp;uuid).await { Err(err) =\u0026gt; { let mut err_map = err_map_clone.lock().await; err_map.insert(uuid, err); } Ok(value) =\u0026gt; { // ... } } }); } combine_errors(\u0026amp;err_map) } Breaking a mutex by copying it around Function arguments are copied in Go. Its not uncommon to commit the mistake of passing a mutex by copy to multiple goroutines in order to synchronize access to a piece of data, the problem is that a mutex will be copied \u0026ndash; including its internal state \u0026ndash; when passed to a goroutine, which means each goroutine will have each its own mutex and all of them will be able to acquire it at the same time.\nvar a int func CriticalSection(m synx.Mutex) { m.Lock() a += 1 m.Unlock() } func main() { mutex := sync.Mutex{} go CriticalSection(mutex) // mutex is copied go CriticalSection(mutex) // mutex is copied } Rust mutexes are supposed to hold the data they protect instead of acting as a flag and when a variable is passed as argument to a function, we say that the value has been moved and it cannot be accessed using the old variable anymore, for this reason, the code does not compile.\nuse tokio::sync::Mutex; fn main() { let mutex = Mutex::new(0); tokio::spawn(critical_section(mutex)); tokio::spawn(critical_section(mutex)); } async fn critical_section(mutex: Mutex\u0026lt;i32\u0026gt;) { let mut value = mutex.lock().await; *value += 1; } --\u0026gt; src/mutex.rs:21:35 | 19 | let mutex = Mutex::new(0); | ----- move occurs because `mutex` has type `tokio::sync::Mutex\u0026lt;i32\u0026gt;`, which does not implement the `Copy` trait 20 | tokio::spawn(critical_section(mutex)); | ----- value moved here 21 | tokio::spawn(critical_section(mutex)); | ^^^^^ value used here after move The incorrect version does not compile and the correct version is pretty easy to write using reference couting to ensure every task uses the same mutex.\nfn main() { let mutex = Arc::new(Mutex::new(0)); tokio::spawn(critical_section(Arc::clone(\u0026amp;mutex))); tokio::spawn(critical_section(Arc::clone(\u0026amp;mutex))); } async fn critical_section(mutex: Arc\u0026lt;Mutex\u0026lt;i32\u0026gt;\u0026gt;) { let mut value = mutex.lock().await; *value += 1; } Conclusion Rust can not stop us from making every kind of mistake but it does seem like it can help us avoid at least some concurrency bugs.\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/rust_compile_time_safety_1/","summary":"Uber has adopted Go as its primary programming language for developing microservices and has a post on its blog called Data Race Patterns in Go where they talk about data races found in their Go codebase.\nI was reading it and thought to myself that many of the problems presented in the post would not even compile in Rust. Can Rust help us avoid writing code with common data races?","title":"Do Go programs with common data races compile in Rust?"},{"content":"What is a log A log is just a immutable sequence of records wih strong ordering semantics that can be used to provide durability, replication and to model consensus. It is usually a 0 indexed file that new entries are appended to because expensive disk seeks can usually be avoided when appending to a file1.\nNot to be confused with the type of logs most people are used to: application logs that are meant to be read by humans although application logs are a degenerative case of the log we are talking about2.\nWhat i called a record is a entry in the log. The entry can be anything in any format.\nYou have seen a log before Databases Postgres uses a write-ahead log to ensure data is not lost if a crash happens3, to enable replication and change data capture. Tables and indexes are modified only after the change been written to the log in which case if a crash happens, the log can be used to go back to a valid state.\nDatomic takes it to the next level by being a log-centric database4.\nFile systems Some file systems known as journaling file systems5 write changes to a log before actually applying them to the internal file system structures to enable crash recovery and avoid data corruption.\nDistributed systems Distributed systems such as Kafka which considers a message as accepted by the cluster after the quorum of in-sync replicas(configuration dependent) have written the message to their log6\nConsensus Consensus algorithms such as Raft7 aka replicated state machines8.\nOperating systems: Three easy pieces - Hard Disk Drives\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI Love logs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPostgres write-ahead log\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRich Hickey: Deconstructing the Database\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJournaling file-system\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKafka: The Definitive Guide: Real-Time Data and Stream Processing at Scale\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn Search of an Understandable Consensus Algorithm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReplicated state machines\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/logs/","summary":"What is a log A log is just a immutable sequence of records wih strong ordering semantics that can be used to provide durability, replication and to model consensus. It is usually a 0 indexed file that new entries are appended to because expensive disk seeks can usually be avoided when appending to a file1.\nNot to be confused with the type of logs most people are used to: application logs that are meant to be read by humans although application logs are a degenerative case of the log we are talking about2.","title":"Logs"},{"content":"Contributing for the first time I have been trying to force myself to do harder things lately in order to practice and learn new things. Since i\u0026rsquo;m doing Rust full time now, i thought it would be a good a idea to contribute to the ecosystem, so i went and enabled notifications for a bunch of Rust related projects and for the Rust project itself.\nI thought i would be able to keep up with the notifications. I was wrong. (obviously) I actually go through a few notifications each day in hope to find something to work on.\nFirst tokio contribution Rust supports async await but it does not come with a runtime by default. It is left for the user to define which runtime their program will use and tokio is the most popular one.\nI was going through my notifications as usual and one issue caught my attention: someone wanted to add a method to get the address the UdpSocket is connected to.\nIt seemed easy enough so i went and claimed it:\nThe implementation was actually pretty simple since mio already had a method that does exact what i needed.\nFrom mio Github repository: Mio is a fast, low-level I/O library for Rust focusing on non-blocking APIs and event notification for building high performance I/O apps with as little overhead as possible over the OS abstractions.\nEverything went as expected and my change got released on tokio 1.18.0.\nFirst Rust contribution A few days went by and a Rust issue caught my attention: a compiler message was incorrect, it turns out, fixing compiler messages is one of the main ways people start contributing to the Rust compiler.\nAnyway, Rust is known for its nice error messages, it does have good error messages indeed but they come at a development cost. The Rust compiler has several functions and methods just to decide which error message to show the user.\nThe offender It is actually valid to add : after a type variable\nfn foo\u0026lt;T:\u0026gt;(t: T) { t.clone(); } note the : after T\nThe compiler would then complain that Clone is not impleted for T and suggest it to be implemented\nerror[E0599]: no method named `clone` found for type parameter `T` in the current scope --\u0026gt; src/lib.rs:2:7 | 2 | t.clone(); | ^^^^^ method not found in `T` | = help: items from traits can only be used if the type parameter is bounded by the trait help: the following trait defines an item `clone`, perhaps you need to restrict type parameter `T` with it: | 1 | fn foo\u0026lt;T: Clone:\u0026gt;(t: T) { | ~~~~~~~~ note there is an extra : after Clone in the suggestion\nI thought it was easy enough and decided to fix it.\nTesting that the compiler error messages are correct is pretty easy, Rust calls this type of test an ui test.\nAll i needed to do was to add a file containing the code that\u0026rsquo;s supposed to error to the ui folder.\n~src/test/ui/traits/issue-95898.rs // Test for #95898: The trait suggestion had an extra `:` after the trait. // edition:2021 fn foo\u0026lt;T:\u0026gt;(t: T) { t.clone(); //~^ ERROR no method named `clone` found for type parameter `T` in the current scope } fn main() {} ~^ ERROR tells the test runner that the error is expected and the test fails if the error does not occur\nand a .stderr file containing the expected error message\n~src/test/ui/traits/issue-95898.stderr error[E0599]: no method named `clone` found for type parameter `T` in the current scope --\u0026gt; $DIR/issue-95898.rs:5:7 | LL | t.clone(); | ^^^^^ method not found in `T` | = help: items from traits can only be used if the type parameter is bounded by the trait help: the following trait defines an item `clone`, perhaps you need to restrict type parameter `T` with it: | LL | fn foo\u0026lt;T: Clone\u0026gt;(t: T) { | ~~~~~~~~ error: aborting due to previous error For more information about this error, try `rustc --explain E0599`. Note that we expect the suggestion to be correct in the .stderr file\nIt took me some time to get used to the compiler but the fix was really easy thanks to WaffleLapkin who was working on a similar issue.\nSecond tokio contribution tokio has the join! macro that can be used when we want to wait for several futures to complete before doing something.\nThink Promise.all if javascript is your thing.\nasync fn process_something_1() { ... } async fn process_something_2() { ... } #[tokio::main] async fn main() { let (result_1, result_2) = tokio::join!(result_1, result_2); ... } Future based concurrency is a cooperative model, it is pretty easy for one task to monopolize processing time if we are not careful. One way to work around this problem is to not allow a task to run forever without being interrupted by giving each task a budget and force the task to yield control back to the runtime whenever its budget is exceeded.\nTask and Future will be used interchangeably\ntokio does the budget think per task and each time a task interacts with a resource, its budget is decreased until it reaches 0 and control is yielded back to the scheduler.\nEach task starts with a budget of 128 and the budget is consumed when interacting with a resource (a Semaphore, for example)\nuse std::sync::Arc; use std::time::Duration; use tokio::sync::Semaphore; async fn foo() { // Consuming a resource decreases the budget by 1. tokio::time::sleep(Duration).await; } async fn foo() { // Consuming a resource decreases the budget by 1. let _permit = permits.clone().acquire_owned().await.unwrap(); } #[tokio::main] async fn main() { let permits = Arc::new(Semaphore::new(1)); // NOTE: join! creates a new task with a budget of 128 let _ = tokio::join!( foo(), bar(Arc::clone(\u0026amp;permits)), ); } The point of giving a budget to each task to stop bad tasks from starving other tasks but it turns out, it is still possible for one task to starve other tasks because join! polls every future inside the same task which means every future passed to join! shares the same task budget of 128.\nNote that join! creates a new task\nA task can starve other tasks by just consuming the whole budget of the task that invoked join! so by the time the other tasks passed to join! are polled, the budget is already 0 which causes them to yield control back to the runtime.\nuse std::sync::Arc; use tokio::sync::Semaphore; #[tokio::main] async fn main() { let permits = Arc::new(Semaphore::new(1)); // join! polls futures in the order they are passed to it. tokio::join!( // This future will be polled first. non_cooperative_task(Arc::clone(\u0026amp;permits)), // This future will be polled second. poor_little_task(permits) ); } async fn non_cooperative_task(permits: Arc\u0026lt;Semaphore\u0026gt;) { // This future will yield back to the runtime after the loop runs 128 times. loop { let _permit = permits.clone().acquire_owned().await.unwrap(); } } // `non_cooperative_task` has been polled and now it is this futures turn. // The bad thing is that `non_cooperative_task` consumed the whole budget // and there\u0026#39;s nothing left for this future to spend. async fn poor_little_task(permits: Arc\u0026lt;Semaphore\u0026gt;) { loop { // Even though this future should be able to acquire the Semaphore, // acquire_owned().await will return Poll::Pending because the budget of // the current task is 0. let _permit = permits.clone().acquire_owned().await.unwrap(); // This println! never gets to run. println!(\u0026#34;Hello!\u0026#34;) } } Poll is the type returned when the runtime checks if a Future is completed. Poll::pending means the Future is not ready. In this case, the Future is actually ready but since it has no budget to spend, it pretends it isn\u0026rsquo;t ready.\nFirst try At first i thought we would just be able to give each future passed to join! its own budget instead of letting them share the current task budget.\nBy current task budget, i mean the budget of the task that invoked join!\njoin! is implemented as declarative macro\nmacro_rules! join { (@ { // One `_` for each branch in the `join!` macro. This is not used once // normalization is complete. ( $($count:tt)* ) // Normalized join! branches $( ( $($skip:tt)* ) $e:expr, )* }) =\u0026gt; {{ use $crate::macros::support::{maybe_done, poll_fn, Future, Pin}; use $crate::macros::support::Poll::{Ready, Pending}; // Safety: nothing must be moved out of `futures`. This is to satisfy // the requirement of `Pin::new_unchecked` called below. let mut futures = ( $( maybe_done($e), )* ); poll_fn(move |cx| { let mut is_pending = false; $( // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; // Try polling if fut.poll(cx).is_pending() { is_pending = true; } )* if is_pending { Pending } else { Ready(($({ // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; fut.take_output().expect(\u0026#34;expected completed future\u0026#34;) },)*)) } }).await }}; // ===== Normalize ===== (@ { ( $($s:tt)* ) $($t:tt)* } $e:expr, $($r:tt)* ) =\u0026gt; { $crate::join!(@{ ($($s)* _) $($t)* ($($s)*) $e, } $($r)*) }; // ===== Entry point ===== ( $($e:expr),* $(,)?) =\u0026gt; { $crate::join!(@{ () } $($e,)*) }; } So i went on and just gave each future its own budget before polling them.\nmacro_rules! join { ... $( // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; // Try polling if crate::coop::budget(|| fut.poll(cx)).is_pending() { is_pending = true; } )* ... } Note that i added crate::coop::budget\nTurns out this doesn\u0026rsquo;t work. It is still pretty easy to create a future that never yields even though it consumes its whole budget:\n... loop { tokio::join!(sem.acquire()); } The future would spend its budget but not the budget of the surrounding task, causing it to never yield.\nSecond try Each time the task created by join! is polled, poll a different future first so as time goes by, every future gets a chance to make progress.\nI took a look at select! and it is able to do just that (up to 64 branches) so i took note and modified join!.\nmacro_rules! join { ... let mut start = 0; ... // BRANCHES is the number of futures passed to join!. for i in 0..BRANCHES { let branch; #[allow(clippy::modulo_one)] { branch = (start + i) % BRANCHES; } match { $( // $crate::count! will return the number of tokens passed to it // up to 64 tokens. $crate::count!( $($skip)* ) =\u0026gt; { // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; // Try polling if fut.poll(cx).is_pending() { is_pending = true; } } )* } #[allow(clippy::modulo_one)] { start = (start + 1) % BRANCHES; } ... } This actually works but $crate::count! can only count up to 64:\n#[macro_export] #[doc(hidden)] macro_rules! count { () =\u0026gt; { 0 }; (_) =\u0026gt; { 1 }; (_ _) =\u0026gt; { 2 }; (_ _ _) =\u0026gt; { 3 }; ... // up to 64 } aaaand\u0026hellip; join! accepts up to 125 futures without changing the recursion limit so this solution wasn\u0026rsquo;t accepted because it would be a breaking change.\nThird try Start the polling round in a different future each time still seems like a good idea. To bypass $crate::count!\u0026rsquo;s limitation, i decided to use a procedural macro.\nNot actually showing code for this one because it is too long\nTurns out people don\u0026rsquo;t like procedural macros very much and it was not accepted.\nFourth try Still the same solution but implemented in a different way. What if instead of using $crate::count! inside the macro to get the index of a future, we counted up front?\njoin! already does some normalization before actually processing the input, so i modified the normalization branches to pair the future index with the future itself.\nmacro_rules! join { (@ { // One `_` for each branch in the `join!` macro. This is not used once // normalization is complete. ( $($count:tt)* ) // The expression `0+1+1+ ... +1` equal to the number of branches. ( $($total:tt)* ) // Normalized join! branches $( ( $($skip:tt)* ) ( $($branch_index:tt)* ) $e:expr, )* }) =\u0026gt; {{ ... let mut start = 0; ... // BRANCHES is the number of futures passed to join!. for i in 0..BRANCHES { let branch; #[allow(clippy::modulo_one)] { branch = (start + i) % BRANCHES; } $( { const INDEX: u32 = $($branch_index)*; if branch == INDEX { // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; // Try polling if fut.poll(cx).is_pending() { is_pending = true; } } } )* } #[allow(clippy::modulo_one)] { start = (start + 1) % BRANCHES; } ... }}; // ===== Normalize ===== (@ { ( $($s:tt)* ) ( $($n:tt)* ) $($t:tt)* } $e:expr, $($r:tt)* ) =\u0026gt; { // i\u0026#39;m new ▼ $crate::join!(@{ ($($s)* _) ($($n)* + 1) $($t)* ($($s)*) ($($n)*) $e, } $($r)*) }; // ===== Entry point ===== ( $($e:expr),* $(,)?) =\u0026gt; { // i\u0026#39;m new ▼ $crate::join!(@{ () (0) } $($e,)*) }; } It works but could be faster. Say we pass 5 futures to join!, how many times would the if statements that check if it is the future\u0026rsquo;s turn to be polled conditions be checked?\nFifth try (the last one) PR\nThe same idea still, poll a different future first every time, except we avoid checking if statement conditions without necessity.\nmacro_rules! join { (@ { // One `_` for each branch in the `join!` macro. This is not used once // normalization is complete. ( $($count:tt)* ) // The expression `0+1+1+ ... +1` equal to the number of branches. ( $($total:tt)* ) // Normalized join! branches $( ( $($skip:tt)* ) $e:expr, )* }) =\u0026gt; {{ use $crate::macros::support::{maybe_done, poll_fn, Future, Pin}; use $crate::macros::support::Poll::{Ready, Pending}; // Safety: nothing must be moved out of `futures`. This is to satisfy // the requirement of `Pin::new_unchecked` called below. let mut futures = ( $( maybe_done($e), )* ); // Each time the future created by poll_fn is polled, a different future will be polled first // to ensure every future passed to join! gets a chance to make progress even if // one of the futures consumes the whole budget. // // This is number of futures that will be skipped in the first loop // iteration the next time. let mut skip_next_time: u32 = 0; poll_fn(move |cx| { const COUNT: u32 = $($total)*; let mut is_pending = false; let mut to_run = COUNT; // The number of futures that will be skipped in the first loop iteration. let mut skip = skip_next_time; skip_next_time = if skip + 1 == COUNT { 0 } else { skip + 1 }; // This loop runs twice and the first `skip` futures // are not polled in the first iteration. loop { $( if skip == 0 { if to_run == 0 { // Every future has been polled break; } to_run -= 1; // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; // Try polling if fut.poll(cx).is_pending() { is_pending = true; } } else { // Future skipped, one less future to skip in the next iteration skip -= 1; } )* } if is_pending { Pending } else { Ready(($({ // Extract the future for this branch from the tuple. let ( $($skip,)* fut, .. ) = \u0026amp;mut futures; // Safety: future is stored on the stack above // and never moved. let mut fut = unsafe { Pin::new_unchecked(fut) }; fut.take_output().expect(\u0026#34;expected completed future\u0026#34;) },)*)) } }).await }}; } Thanks Honestly, i had a lot of help from Darksonn.\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/contributing_to_tokio/","summary":"Contributing for the first time I have been trying to force myself to do harder things lately in order to practice and learn new things. Since i\u0026rsquo;m doing Rust full time now, i thought it would be a good a idea to contribute to the ecosystem, so i went and enabled notifications for a bunch of Rust related projects and for the Rust project itself.\nI thought i would be able to keep up with the notifications.","title":"Contributing to Rust and tokio"},{"content":"Why Rc cannot be sent between threads We get a compile error if we try to send Rc\u0026lt;T\u0026gt; to another thread:\nuse std::rc::Rc; fn main() { let rc = Rc::new(1); std::thread::spawn(|| { println!(\u0026#34;{}\u0026#34;, *rc); }) .join(); } error[E0277]: `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely --\u0026gt; src/main.rs:5:3 | 5 | std::thread::spawn(|| { | ^^^^^^^^^^^^^^^^^^ `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely | = help: the trait `Sync` is not implemented for `Rc\u0026lt;i32\u0026gt;` = note: required because of the requirements on the impl of `Send` for `\u0026amp;Rc\u0026lt;i32\u0026gt;` = note: required because it appears within the type `[closure@src/main.rs:5:22: 7:4]` note: required by a bound in `spawn` --\u0026gt; /home/bruno/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:625:8 | 625 | F: Send + \u0026#39;static, | ^^^^ required by this bound in `spawn` For more information about this error, try `rustc --explain E0277`. The compile error is triggered because the closure passed to std::thread::spawn must be Send. Types that implement Send are types that can be transferred across thread boundaries.\nRc primer Rc\u0026lt;T\u0026gt; is a smart pointer that can be used to hold multiple references to T when T is owned by several objects. It kinda looks like a std::shared_ptr from C++ except it does not increment and decrement the reference count atomically, if you need thread safe reference counting in Rust take a look at Arc.\nA value contained in an Rc\u0026lt;T\u0026gt; will be dropped when the last reference to it is dropped. It is possible to create reference cycles and leak memory as well.\nIf we need a new reference to a T, the Rc\u0026lt;T\u0026gt; can just be cloned:\nlet a = Rc::new(1); let b = Rc::clone(\u0026amp;a); Rc internals If we take a look at the Rc\u0026lt;T\u0026gt; source code we will see that it is actually kinda simple:\npub struct Rc\u0026lt;T: ?Sized\u0026gt; { ptr: NonNull\u0026lt;RcBox\u0026lt;T\u0026gt;\u0026gt;, phantom: PhantomData\u0026lt;RcBox\u0026lt;T\u0026gt;\u0026gt;, } ?Sized means the size of T does not need to be known at compile-time. It\u0026rsquo;s fine to accept a T that\u0026rsquo;s not Sized because Rc\u0026lt;T\u0026gt; is Sized.\nA Rc\u0026lt;T\u0026gt; pretty much boils down to a struct with two counters and a pointer to a value of type T. In the Rc\u0026lt;T\u0026gt; source code, the struct is called RcBox\u0026lt;T\u0026gt;:\n// Note that Cell is used for internal mutability. struct RcBox\u0026lt;T: ?Sized\u0026gt; { /// How many references we have to this value. strong: Cell\u0026lt;usize\u0026gt;, // Weak ref? We\u0026#39;ll ignore it for now. weak: Cell\u0026lt;usize\u0026gt;, /// The actual value value: T, } When a Rc\u0026lt;T\u0026gt; is created, its strong count will be 1 because there is only one reference to the value inside of it. If we need more references (the point of using Rc\u0026lt;T\u0026gt;) we can just clone the Rc\u0026lt;T\u0026gt;\nlet a: Rc\u0026lt;String\u0026gt; = Rc::new(String::from(\u0026#34;hello world\u0026#34;)); let b: Rc\u0026lt;String\u0026gt; = a.clone(); Cloning an Rc\u0026lt;T\u0026gt; means increasing its strong count and creating a copy of the RcBox\u0026lt;T\u0026gt;.\nimpl\u0026lt;T: ?Sized\u0026gt; Clone for Rc\u0026lt;T\u0026gt; { #[inline] fn clone(\u0026amp;self) -\u0026gt; Rc\u0026lt;T\u0026gt; { unsafe { // inner() returns the \u0026amp;RcBox\u0026lt;T\u0026gt; that\u0026#39;s in the Rc\u0026lt;T\u0026gt; struct. self.inner().inc_strong(); Self::from_inner(self.ptr) } } } inc_strong literally just increments the strong counter besides some safety checks:\n#[inline] fn inc_strong(\u0026amp;self) { let strong = self.strong(); // We want to abort on overflow instead of dropping the value. // The reference count will never be zero when this is called; // nevertheless, we insert an abort here to hint LLVM at // an otherwise missed optimization. if strong == 0 || strong == usize::MAX { abort(); } self.strong_ref().set(strong + 1); } and from_inner just copies the pointer to RcBox\u0026lt;T\u0026gt;:\nunsafe fn from_inner(ptr: NonNull\u0026lt;RcBox\u0026lt;T\u0026gt;\u0026gt;) -\u0026gt; Self { Self { ptr, phantom: PhantomData } } After the clone, this is how things look like:\nThe strong count is decremented in the Rc\u0026lt;T\u0026gt; Drop implementation and the memory is freed if there\u0026rsquo;s no references left.\nunsafe impl\u0026lt;#[may_dangle] T: ?Sized\u0026gt; Drop for Rc\u0026lt;T\u0026gt; { fn drop(\u0026amp;mut self) { unsafe { self.inner().dec_strong(); if self.inner().strong() == 0 { // destroy the contained object ptr::drop_in_place(Self::get_mut_unchecked(self)); // remove the implicit \u0026#34;strong weak\u0026#34; pointer now that we\u0026#39;ve // destroyed the contents. self.inner().dec_weak(); if self.inner().weak() == 0 { Global.deallocate( self.ptr.cast(), Layout::for_value(self.ptr.as_ref()) ); } } } } } #[may_dangle] has to do with drop check\nWhy Rc is not Send after all? Every time a Rc\u0026lt;T\u0026gt; is cloned, its strong count is incremented. If we had two or more threads trying to clone a Rc\u0026lt;T\u0026gt; at the same time, there would be a race condition since access to the strong count that\u0026rsquo;s in the RcBox\u0026lt;T\u0026gt; is not synchronized.\nReferences https://github.com/rust-lang/rust\nhttps://doc.rust-lang.org/std/marker/trait.Send.html\nhttps://doc.rust-lang.org/nomicon/send-and-sync.html\nhttps://doc.rust-lang.org/std/rc/struct.Rc.html\nhttps://doc.rust-lang.org/std/sync/struct.Arc.html\nhttps://doc.rust-lang.org/std/ptr/struct.NonNull.html\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/why_rc_is_not_send/","summary":"Why Rc cannot be sent between threads We get a compile error if we try to send Rc\u0026lt;T\u0026gt; to another thread:\nuse std::rc::Rc; fn main() { let rc = Rc::new(1); std::thread::spawn(|| { println!(\u0026#34;{}\u0026#34;, *rc); }) .join(); } error[E0277]: `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely --\u0026gt; src/main.rs:5:3 | 5 | std::thread::spawn(|| { | ^^^^^^^^^^^^^^^^^^ `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely | = help: the trait `Sync` is not implemented for `Rc\u0026lt;i32\u0026gt;` = note: required because of the requirements on the impl of `Send` for `\u0026amp;Rc\u0026lt;i32\u0026gt;` = note: required because it appears within the type `[closure@src/main.","title":"Why Rc\u003cT\u003e is not Send"},{"content":"Intro Token bucket is an algorithm that can be used to rate limit requests made or received by a service.\nHow it works The algorithm is called token bucket because of the way it works: imagine we have a bucket with x tokens where each accepted request consumes one token from the bucket and a token is added back to the bucket at an interval.\nA bucket with 1 token that is refilled each second means the service accepts one request per second.\nA bucket with 5 tokens where a token is added to the bucket every 1/5 seconds means the service accepts 5 requests per second.\nA bucket with x tokens where a token is added to the bucket every 1/x seconds means the service accepts x requests per second.\nRequests that are received when the bucket is empty can just be dropped or enqueued to be handled later.\nImplementation in Rust The source code can be found here.\nThe bucket will accept x requests per second.\n#[derive(Debug)] struct Config { /// The number of requests that can be accepted every second. requests_per_second: usize, } #[derive(Debug)] struct Bucket { config: Config, /// How many requests we can accept at this time. tokens: AtomicUsize, /// Sends are actually never made in this channel. /// It is used only for the worker thread to know when the bucket /// has been dropped and exit. close_channel_sender: Sender\u0026lt;()\u0026gt;, } A thread is spawned to refill the bucket every 1/Config::requests_per_second, at this rate the bucket will accept around Config::requests_per_second requests per second.\nimpl Bucket { pub fn new(config: Config) -\u0026gt; Arc\u0026lt;Self\u0026gt; { let (sender, receiver) = crossbeam_channel::unbounded::\u0026lt;()\u0026gt;(); let tokens = AtomicUsize::new(1); let bucket = Arc::new(Self { config, tokens, close_channel_sender: sender, }); let bucket_clone = Arc::downgrade(\u0026amp;bucket); std::thread::spawn(move || Bucket::add_tokens_to_bucket_on_interval( bucket_clone, receiver ) ); bucket } fn add_tokens_to_bucket_on_interval(bucket: Weak\u0026lt;Bucket\u0026gt;, receiver: Receiver\u0026lt;()\u0026gt;) { let interval = { match bucket.upgrade() { None =\u0026gt; { error!( \u0026#34;unable to define interval to add tokens to bucket because bucket has been dropped\u0026#34; ); return; } Some(bucket) =\u0026gt; Duration::from_secs_f64(1.0 / (bucket.config.requests_per_second as f64)), } }; debug!(?interval, \u0026#34;will add tokens to bucket at interval\u0026#34;); let ticker = crossbeam_channel::tick(interval); loop { select! { recv(ticker) -\u0026gt; _ =\u0026gt; match bucket.upgrade() { None =\u0026gt; { debug!(\u0026#34;cannot upgrade Weak ref to Arc, exiting\u0026#34;); return; } Some(bucket) =\u0026gt; { let _ = bucket .tokens .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |tokens| { Some(std::cmp::min(tokens + 1, bucket.config.requests_per_second)) }); } }, recv(receiver) -\u0026gt; message =\u0026gt; { // An error is returned when we try to received from a // channel that has been closed and this channel // will only be closed when the bucket is dropped. if message == Err(RecvError) { debug!(\u0026#34; bucket has been dropped, won\u0026#39;t try to add tokens to the bucket anymore\u0026#34; ); return; } } } } } } And a function can be called to find out if there\u0026rsquo;s enough tokens in the bucket to accept a request. A token is consumed if the request is accepted.\nimpl Bucket { ... /// Returns true if there\u0026#39;s enough tokens in the bucket. pub fn acquire(\u0026amp;self) -\u0026gt; bool { self .tokens .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |tokens| { Some(if tokens \u0026gt; 0 { tokens - 1 } else { tokens }) }) .map(|tokens_in_the_bucket| tokens_in_the_bucket \u0026gt; 0) .unwrap_or(false) } } References https://en.wikipedia.org/wiki/Token_bucket\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/token_bucket/","summary":"Intro Token bucket is an algorithm that can be used to rate limit requests made or received by a service.\nHow it works The algorithm is called token bucket because of the way it works: imagine we have a bucket with x tokens where each accepted request consumes one token from the bucket and a token is added back to the bucket at an interval.\nA bucket with 1 token that is refilled each second means the service accepts one request per second.","title":"Token bucket"},{"content":"Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.\nThe authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus\nLeader election Log replication Safety and enforces a stronger degree of coherency to reduce the number of states that must be considered.\nRaft also includes a new mechanism for changing cluster membership.\nWhat is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.\nWhy Raft was created Paxos, another consensus algorithm, is the most popular algorithm when talking about consensus algorithms. Most implementations of consensus are based on Paxos or influenced by it.\nThe problem is that Paxos is hard to understand and its architecture does not support pratical systems without modifications.\nThe point of Raft is to be a consensus algorithm that is as efficient as Paxos but easier to understand and implement.\nWhat is unique about Raft Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov\u0026rsquo;s Viewstamped Replication), but it has several new features:\nStrong leader: Raft uses a stronger form of leadership than other consensus algorithms. For example, log entries only flow from the leader to other servers. This simplifies the management of the replicated log.\nLeader election: Raft uses randomized timers to elect leaders. This makes conflicts easier and faster to resolve.\nMembership changes: Raft\u0026rsquo;s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.\nRaft in the wild https://github.com/tikv/raft-rs\nhttps://github.com/hashicorp/raft\nhttps://github.com/sofastack/sofa-jraft\nhttps://github.com/eBay/NuRaft\nReplicated state machines States machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems.\nHow the consensus algorithms work with replicated state machines Replicated state machines are typically implemented using a replicated log.\nThe consensus algorithm manages a replicated log containg state machine commands from clients. The state machines process identical sequences of commands fro the logs, so they produce the same outputs.\nThe consensus algorithm module on the server receives commands from clients and adds them to its log. It communicates with the consensus algorithm modules on other servers to ensure that every log eventually contains the commands in the same order even if some of the servers fail.\nOnce commands are properly replicated (every server has the same commands in the same order), each server\u0026rsquo;s state machine processes them and the outputs are returned to the clients. As a result, the servers appear to form a single, highly reliable state machine.\nTypical properties of consensus algorithms Safety: They ensure that an incorrect result is never returned under all non-Byzantine conditions, including networks delays, partitions, and packet lost, duplication and reordering.\nAvailability: They are fully functional as the majority of the servers are functional. For example, a cluster of five servers can tolerate the failure of two servers. Servers that failed may rejoin the cluster after recovering.\nConsistency: They do not depend on timing to ensure the consistency of the logs.\nPerformance: A command can complete as soon as a majority of the cluster has responded has accepted the command.\nExamples of replicated state machines https://www.cs.cornell.edu/courses/cs6464/2009sp/lectures/16-chubby.pdf\nhttps://github.com/apache/zookeeper\nPaxos Created by Leslie Lamport, Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry. This is known as single-decree Paxos. Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log. This is known as multi-Paxos.\nPaxos criticism Paxos is difficult to understand. The full explanation is opaque and because of that a lot of effort is necessary to understand it and only a few people succeed in understanding it.\nBecause of its difficulty, there have been several attempts to simplify Paxos explanation. These explanation focus on the single-decree subset and even then, they are still hard to understand.\nThe second problem with Paxos is that there is no widely agreed-upon algorithm for multi-Paxos. Lamport\u0026rsquo;s descriptions are mostly about single-decree Paxos.\nThe third problem is that Paxos uses a peer-to-peer approach at its core. If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions.\nBecause of these problems, pratical systems implementations begin with Paxos, discover the difficulties in implementing, and then develop a significantly different architecture.\nThe Raft consensus algorithm Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. A leader can fail or become disconnected from the other servers, in which case a new leader is elected.\nRaft decomposes the consensus problem into three subproblems:\nLeader election: A new leader must be chosen when an existing leader fails.\nLog replication: The leader must accept log entries from clients and replicate them accross the cluster, forcing the other logs to agree with its own.\nSafety: If any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.\nRaft basics A raft cluster contains several servers, five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader, follower or candidate.\nleader: In normal operation there is exactly one leader and all of the other servers are followers. The leader handles all client requests.\nfollower: Followers simply respond to requests from leaders and candidates, if a client contacts a follower, the request is redirected to the leader.\ncandidate: A candidate is a follower that wants to become a leader.\nRaft divides time into terms of arbitraty length numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. If the election results in a split vote, the term ends with no leader and a new term with a new election begins shortly.\nEach server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate and if one server\u0026rsquo;s current term is smaller than the other\u0026rsquo;s, then it updates its current term to the larger value. If a candidate or leader discovers that is term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request.\nRaft servers communication with remote procedure calls Raft servers coomunicate using remote procedure calls. The basic consensus algorithm requires two types of RPCs:\nRequestVote: RequestVote RPCs are initiated by candidates during elections.\nAppendEntries: AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat.\nServers retry RPCs if they do not receive a response in a timely manner.\nLeader election Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers and remain in the follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader.\nTo begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each server in the cluster. A candidate continues in the candidate state until one of three things happens:\nWins election: The candidate wins the election and becomes the new leader.\nA candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server votes for at most one candidate in a given term, on a first-come-first-served basis. Once a candidate wins an election, it becomes the leader and sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.\nOther server wins election: Another candidate wins the election and becomes the new leader.\nDuring an election, a candidate may receive an AppendEntries RPC from another server claiming to be the leader. If the leader\u0026rsquo;s term is at least as large as the candidate\u0026rsquo;s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the request term is smaller than the candidate\u0026rsquo;s current term, the request is rejected as described in Raft basics.\nElection timeout: A period of time goes by with no winner.\nIf many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by increasing its term and iniating another round of RequestVote RPCs.\nElections timeouts are chosen randomly from the range like 150..300ms to ensure that split votes are rare and that hey are resolved quickly.\nEach candidate gets a randomized election timeout at the start of an election, and it waits for the timeout to elapse before starting a new election.\nElection restriction Raft uses the voting process to prevent a candidate from winning an election unless its log contains all commited entries from previous terms. When a RequestVote RPC is made, the candidate includes the index and term of the last entry in its log, the server that receives the request (aka the voter) denies the request if its own log is more up-to-date than of the candidate. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.\nLog replication Once a leader has been elected, it begins servicing client requests that each contain a command to be executed by the replicated state machines. When a request is received, the leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the servers to replicate the entry. Only after the entry has been replicated, the leader applies the entry to its state machine and returns the result of that execution to the client. The leader retries AppendEntries RPCs until all followers eventually store all log entries.\nLogs are composed of sequentially numbered entries. Each entry contains the term in which it was created and a command for the state machine. The leader decides when it is safe to apply a log entry to the state machines, entries that have been applied to the state machines are called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is commited once the leader that created the entry has replicated it on a majority of the servers. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs so that other server eventually find out. Once a follower learns that a log entry is committed, it applies every entry up to the entry to its local state machine.\ncommand term │ │ │ │ 0 1 2 3 4 5 6 7 log index │ │ ┌────┬────┬────┬────┬────┬────┬────┬────┐ │ ├─► 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ 3 │ │ └──────┴─►x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │ leader └────┴────┴────┴────┴────┴────┴────┴────┘ ┌────┬────┬────┬────┬────┐ │ 1 │ 1 │ 1 │ 2 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │ │ └────┴────┴────┴────┴────┘ │ │ ┌────┬────┬────┬────┬────┬────┬────┬────┐ │ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │ └────┴────┴────┴────┴────┴────┴────┴────┘ │ │ followers ┌────┬────┐ │ │ 1 │ 1 │ │ │x=3 │y=1 │ │ └────┴────┘ │ │ ┌────┬────┬────┬────┬────┬────┬────┐ │ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │ │ └────┴────┴────┴────┴────┴────┴────┘ │ │ └──────────────────────────────────┘ committed entries Raft mantains the following properties:\nIf two entries in different logs have the same index and term, then they store the same command.\nIf two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\nHow log inconsistencies are handled TLDR: Leader overwrites follower logs if they are out of sync.\nIn Raft, the leader handles inconsistencies by forcing the follower\u0026rsquo;s logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader\u0026rsquo;s log. To bring a follower\u0026rsquo;s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower\u0026rsquo;s log after that point, and send the follower all of the leader\u0026rsquo;s entries after that point. The leader maintains a nextIndex for each follower, which is the index of the next log the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log.\nSafety TLDR: Only servers that contain all of the entries commited in previous terms may become leader at any given term.\nThere\u0026rsquo;s a problem with the Raft description so far:\nFor example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones. As a result, different state machines might execute different command sequences. This is fixed by adding a restriction on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries commited in previous terms.\nRestriction on committing logs Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader\u0026rsquo;s current term are commited by counting replicas.\nFollower and candidate crashes If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely, if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCS are idempotent, so this causes no harm. If a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.\nTiming and availability Raft will be able to elect and maintain a steady leader as long as the system satifies the following timing requirement:\nbroadcast_time \u0026lt;= election_timeout \u0026lt;= MTBF\nbroad_cast_time is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses.\nelection_timeout is the election timeout described in Leader election.\nMTBF is the average time between failures for a single server.\nThe broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbet messages required to keep followers from starting elections.\nThe election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.\nCluster membership changes Configuration changes are incorporated into the Raft consensus algorithm. In Raft the cluster first switches to a transitional configuration called joint consensus, once the joint consensus has been committed, the system then transitions to the new configuration.\nThe join consensus combines both the old an new configurations:\nLog entries are replicated to all servers in both configurations. Any server from either configuration may serve as leader. Agreement for elections and entry commitment requires separate majorities from both the old and new configurations. The joint consensus allows individual servers to transition between configurations at different times without compromising safety.\nHow cluster configurations are stored and communicated Cluster configurations are stored and communicated using special entries in the replicated log. When the leader receives a request to change the configuration from config_old (the current configuration) to config_new (the new configuration), it stores a tuple (config_old, config_new) as a log entry and replicates that entry. Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions even if the entry has not been committed yet.\nLog compaction TLDR: We don\u0026rsquo;t have infinite memory, discard logs that aren\u0026rsquo;t needed anymore.\nRaft\u0026rsquo;s log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound. As the log grows longer, it occupies more spaces and takes more time to replay.\nSnapshotting is the simplest approach to compaction. In snapshotting the entire current system state is written to a snapshot on stable storage, then the tire log up to that point is discarded. Snapshotting is used in Chubby and ZooKeeper and in Raft as well.\nThe basic idea of snapshotting in Raft: TLDR: Add the current machine state to the log and delete all logs used to get to this state. The snapshot should also be written to stable storage.\nEach server takes snapshots independently, covering just the commited entries in its log. Most of the work consists of the state machine writing its current state to the snapshot. Raft also includes a small amount of metadata in the snapshot: the last included index which is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied), and the last included term which is the term of the entry. The snapshot also includes the latest configuration. Once a server completes a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.\n0 1 2 3 4 5 6 log index ┌────┬────┬────┬────┬────┬────┬────┐ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ │ before snapshot │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │ │ └────┴────┴────┴────┴────┴────┴────┘ snapshot ┌────────────────────────┬────┬────┐ │last included index: 4 │ 3 │ 3 │ │ │last included term: 3 │y=7 │x=5 │ │ │state machine state: ├────┴────┘ │ after snapshot │ x = 0 │ │ │ y = 9 │ │ └────────────────────────┘ │ │ └────────────────────────┘ committed entries When a new follower joins the cluster The way to bring a new foller up-to-date is for the leader to send it a snapshot over the network. The leader uses a new RPC called InstallSnapShot to send snapshots to followers that are too far behind because they are new or because they are too slow.\nWhen a follower receives a snapshot with this RPC:\nIf the snapshot contains new information on in the follower\u0026rsquo;s log, the follower discards and replaces its log with the snapshot.\nIf the snapshot contains only a prefix of its log, then log entries covered by the snapshot are deleted bu entries following the snapshot are still valid and must be retained.\nClient interaction Clients of Raft send all of their requests to the leader.\nHow clients find the leader When a client starts up, it connects to a randomly-chosen server. If the client\u0026rsquo;s choice is not the leader, that server will reject the client\u0026rsquo;s request and supply information about the most recent leader it has heard from.\nCommand deduplication For example, if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing to be executed a second time.\nThe solution is for clients to assign unique serial numbers to every command. Then, the state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number ha already been executed, it responses immediatelly without re-executing the request.\nRead-only operations should not return stale data When a leader responds to a request, its log could be outdated with a new leader had been elected in the meantime.\nTo avoid this situation:\nA leader must have the latest information on which entries are committed. Because of that, each leader commits a blank no-op entry into the log at the start of its term.\nA leader must check whether it has been deposed before processing a read-only request. Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only request.\nReferences In Search of an Understandable Consensus Algorithm (Extended Version) - https://raft.github.io/raft.pdf\nDesigning for Understandability: The Raft Consensus Algorithm - https://www.youtube.com/watch?v=vYp4LYbnnW8 \u0026ldquo;Raft - The Understandable Distributed Protocol\u0026rdquo; by Ben Johnson (2013) - https://www.youtube.com/watch?v=ro2fU8_mr2w\nhttps://github.com/hashicorp/raft\nMIT 6.824: Distributed Systems (Spring 2020) Lecture 6: Fault Tolerance: Raft - https://www.youtube.com/watch?v=64Zp3tzNbpE\u0026amp;list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB\u0026amp;index=6\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/raft_notes/","summary":"Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.\nThe authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus\nLeader election Log replication Safety and enforces a stronger degree of coherency to reduce the number of states that must be considered.\nRaft also includes a new mechanism for changing cluster membership.\nWhat is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.","title":"Notes taken from the Raft paper"},{"content":"What\u0026rsquo;s a Bloom filter A bloom filter is a data-structure that can be used to check if a set contains an element. It uses way less memory than a conventional set data-structure by sacrificing accuracy.\nExample Say we are building a log-structured merge-tree, we can use a bloom filter to find out if the LSM-tree contains a particular key in O(1) time in most cases, the downside is that sometimes the bloom filter would say that the LSM-tree contains a key, but it actually does not and we would go searching for the value that\u0026rsquo;s mapped to the key and never actually find it.\nIt is used in a lot of places.\nHow it works A bloom filter is just a bit-set that uses n deterministic hash functions to add elements to it.\nempty bit-set\nAdding elements to the set To add the key bob to the set, we run the key through each of the n hash functions and map the hash function output to one of the positions in the bit-set and for each position, we flip the bit to 1.\nbit-set after bob was added to the bloom filter Finding out if the set contains an element To find out if the set contains the key bob, we run the key through each of the n hash functions again \u0026ndash; since the hash functions must be deterministic they will always map to the same position in the bit-set \u0026ndash; and check if the bit is set to 1 for each of the bit-set positions we reached after running the key through the hash functions. If every hash function maps to a bit set to 1, it means the key is in the set.\nbob is in the set because every hash function mapped it to a bit set to 1 alice is not in the set because not every hash function mapped to a bit set to 1 False positives Since collisions can happen some keys will be mapped to bits that were set to 1 when other keys were added to the set. In this case, the bloom filter will say that it contains the key even though it does not.\nthe bit-set after bob was added to it since john maps to the same bits as bob and the bits were set to 1 after bob was added to the set, we got a false positive Removing an element from the set As it stands, removing an element from the set is not actually possible. If we had a bloom filter that uses 3 hash functions that looks like this after adding alice and bob to it:\nbloom filter after adding alice and bob to it with 3 hash functions Note that alice and bob hash to the same position in the bit-set for some of the hash functions\nbits shared between alice and bob are in white The naive solution is to remove alice from the bloom filter by setting the bits mapped by hashi(alice) to 0:\nbits that were flipped to 0 are in white Now, let\u0026rsquo;s check if alice is in the set, for a key to be in the set hashi(key) must map to bits set to 1\nhashi(alice) maps to bits set to 0 which means alice is not in the set alice is not in the set as expected. Let\u0026rsquo;s see if bob is still in the set, it should be since we didn\u0026rsquo;t remove it.\nnot every hashi(bob) maps to bits set to 1 which means bob is not in the set, bits set to 0 after removing alice from the set are in white bob is not in the set anymore, even though we didn\u0026rsquo;t remove it. The problem is that since keys may share the positions in the bit-set, we cannot just flip bits back to 0 to remove a key from the set because in doing so we may flip bits that are used by other keys.\nCounting bloom filter Since we cannot flip bits back to 0 to remove a key from the set, we could maintain a counter instead of a single bit. When a key is added to the set, the counter is incremented and when a key is removed from the set, the counter is decremented. If the counter reaches 0, it means no keys are mapped to the position.\nPositions that have more than one key mapped to it will have a counter greater than 1.\npositions in white are shared between two or more keys and have a counter greater than 1 removing alice from the set by decrementing the counters mapped by hashi(alice) After decrementing the counters, not every hashi(alice) maps to a counter greater than 0 which means alice is not in the set anymore. Unlike the bloom filter that uses only bits, hashi(bob) still maps to counters that are greater than 0 which means bob is still in the set.\nExample in Rust https://github.com/PoorlyDefinedBehaviour/bloom_filter/\nReferences Network Applications of Bloom Filters: A Survey - https://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems - Martin Kleppmann\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/bloom_filter/","summary":"What\u0026rsquo;s a Bloom filter A bloom filter is a data-structure that can be used to check if a set contains an element. It uses way less memory than a conventional set data-structure by sacrificing accuracy.\nExample Say we are building a log-structured merge-tree, we can use a bloom filter to find out if the LSM-tree contains a particular key in O(1) time in most cases, the downside is that sometimes the bloom filter would say that the LSM-tree contains a key, but it actually does not and we would go searching for the value that\u0026rsquo;s mapped to the key and never actually find it.","title":"Bloom filter"}]