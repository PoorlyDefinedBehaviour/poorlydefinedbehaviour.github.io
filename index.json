[{"content":"Why Rc cannot be sent between threads We get a compile error if we try to send Rc\u0026lt;T\u0026gt; to another thread:\nuse std::rc::Rc;  fn main() {  let rc = Rc::new(1);  std::thread::spawn(|| {  println!(\u0026#34;{}\u0026#34;, *rc);  })  .join(); }  error[E0277]: `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely  --\u0026gt; src/main.rs:5:3  | 5 | std::thread::spawn(|| {  | ^^^^^^^^^^^^^^^^^^ `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely  |  = help: the trait `Sync` is not implemented for `Rc\u0026lt;i32\u0026gt;`  = note: required because of the requirements on the impl of `Send` for `\u0026amp;Rc\u0026lt;i32\u0026gt;`  = note: required because it appears within the type `[closure@src/main.rs:5:22: 7:4]` note: required by a bound in `spawn`  --\u0026gt; /home/bruno/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:625:8  | 625 | F: Send + \u0026#39;static,  | ^^^^ required by this bound in `spawn`  For more information about this error, try `rustc --explain E0277`. The compile error is triggered because the closure passed to std::thread::spawn must be Send. Types that implement Send are types that can be transferred across thread boundaries.\nRc primer Rc\u0026lt;T\u0026gt; is a smart pointer that can be used to hold multiple references to T when T is owned by several objects. It kinda looks like a std::shared_ptr from C++ except it does not increment and decrement the reference count atomically, if you need thread safe reference counting in Rust take a look at Arc.\n A value contained in an Rc\u0026lt;T\u0026gt; will be dropped when the last reference to it is dropped. It is possible to create reference cycles and leak memory as well.\n If we need a new reference to a T, the Rc\u0026lt;T\u0026gt; can just be cloned:\nlet a = Rc::new(1); let b = Rc::clone(\u0026amp;a); Rc internals If we take a look at the Rc\u0026lt;T\u0026gt; source code we will see that it is actually kinda simple:\npub struct Rc\u0026lt;T: ?Sized\u0026gt; {  ptr: NonNull\u0026lt;RcBox\u0026lt;T\u0026gt;\u0026gt;,  phantom: PhantomData\u0026lt;RcBox\u0026lt;T\u0026gt;\u0026gt;, }  ?Sized means the size of T does not need to be known at compile-time. It\u0026rsquo;s fine to accept a T that\u0026rsquo;s not Sized because Rc\u0026lt;T\u0026gt; is Sized.\n A Rc\u0026lt;T\u0026gt; pretty much boils down to a struct with two counters and a pointer to a value of type T. In the Rc\u0026lt;T\u0026gt; source code, the struct is called RcBox\u0026lt;T\u0026gt;:\n// Note that Cell is used for internal mutability. struct RcBox\u0026lt;T: ?Sized\u0026gt; {  /// How many references we have to this value.  strong: Cell\u0026lt;usize\u0026gt;,  // Weak ref? We\u0026#39;ll ignore it for now.  weak: Cell\u0026lt;usize\u0026gt;,  /// The actual value  value: T, } When a Rc\u0026lt;T\u0026gt; is created, its strong count will be 1 because there is only one reference to the value inside of it. If we need more references (the point of using Rc\u0026lt;T\u0026gt;) we can just clone the Rc\u0026lt;T\u0026gt;\nlet a: Rc\u0026lt;String\u0026gt; = Rc::new(String::from(\u0026#34;hello world\u0026#34;)); let b: Rc\u0026lt;String\u0026gt; = a.clone(); Cloning an Rc\u0026lt;T\u0026gt; means increasing its strong count and creating a copy of the RcBox\u0026lt;T\u0026gt;.\nimpl\u0026lt;T: ?Sized\u0026gt; Clone for Rc\u0026lt;T\u0026gt; {  #[inline]  fn clone(\u0026amp;self) -\u0026gt; Rc\u0026lt;T\u0026gt; {  unsafe {  // inner() returns the \u0026amp;RcBox\u0026lt;T\u0026gt; that\u0026#39;s in the Rc\u0026lt;T\u0026gt; struct.  self.inner().inc_strong();  Self::from_inner(self.ptr)  }  } } inc_strong literally just increments the strong counter besides some safety checks:\n #[inline] fn inc_strong(\u0026amp;self) {  let strong = self.strong();   // We want to abort on overflow instead of dropping the value.  // The reference count will never be zero when this is called;  // nevertheless, we insert an abort here to hint LLVM at  // an otherwise missed optimization.  if strong == 0 || strong == usize::MAX {  abort();  }  self.strong_ref().set(strong + 1); } and from_inner just copies the pointer to RcBox\u0026lt;T\u0026gt;:\nunsafe fn from_inner(ptr: NonNull\u0026lt;RcBox\u0026lt;T\u0026gt;\u0026gt;) -\u0026gt; Self {  Self { ptr, phantom: PhantomData } } After the clone, this is how things look like:\nThe strong count is decremented in the Rc\u0026lt;T\u0026gt; Drop implementation and the memory is freed if there\u0026rsquo;s no references left.\nunsafe impl\u0026lt;#[may_dangle] T: ?Sized\u0026gt; Drop for Rc\u0026lt;T\u0026gt; {  fn drop(\u0026amp;mut self) {  unsafe {  self.inner().dec_strong();  if self.inner().strong() == 0 {  // destroy the contained object  ptr::drop_in_place(Self::get_mut_unchecked(self));   // remove the implicit \u0026#34;strong weak\u0026#34; pointer now that we\u0026#39;ve  // destroyed the contents.  self.inner().dec_weak();   if self.inner().weak() == 0 {  Global.deallocate(  self.ptr.cast(),  Layout::for_value(self.ptr.as_ref())  );  }  }  }  } }  #[may_dangle] has to do with drop check\n Why Rc is not Send after all? Every time a Rc\u0026lt;T\u0026gt; is cloned, its strong count is incremented. If we had two or more threads trying to clone a Rc\u0026lt;T\u0026gt; at the same time, there would be a race condition since access to the strong count that\u0026rsquo;s in the RcBox\u0026lt;T\u0026gt; is not synchronized.\nReferences https://github.com/rust-lang/rust\nhttps://doc.rust-lang.org/std/marker/trait.Send.html\nhttps://doc.rust-lang.org/nomicon/send-and-sync.html\nhttps://doc.rust-lang.org/std/rc/struct.Rc.html\nhttps://doc.rust-lang.org/std/sync/struct.Arc.html\nhttps://doc.rust-lang.org/std/ptr/struct.NonNull.html\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/why_rc_is_not_send/","summary":"Why Rc cannot be sent between threads We get a compile error if we try to send Rc\u0026lt;T\u0026gt; to another thread:\nuse std::rc::Rc;  fn main() {  let rc = Rc::new(1);  std::thread::spawn(|| {  println!(\u0026#34;{}\u0026#34;, *rc);  })  .join(); }  error[E0277]: `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely  --\u0026gt; src/main.rs:5:3  | 5 | std::thread::spawn(|| {  | ^^^^^^^^^^^^^^^^^^ `Rc\u0026lt;i32\u0026gt;` cannot be shared between threads safely  |  = help: the trait `Sync` is not implemented for `Rc\u0026lt;i32\u0026gt;`  = note: required because of the requirements on the impl of `Send` for `\u0026amp;Rc\u0026lt;i32\u0026gt;`  = note: required because it appears within the type `[closure@src/main.","title":"Why Rc\u003cT\u003e is not Send"},{"content":"Intro Token bucket is an algorithm that can be used to rate limit requests made or received by a service.\nHow it works The algorithm is called token bucket because of the way it works: imagine we have a bucket with x tokens where each accepted request consumes one token from the bucket and a token is added back to the bucket at an interval.\nA bucket with 1 token that is refilled each second means the service accepts one request per second.\nA bucket with 5 tokens where a token is added to the bucket every 1/5 seconds means the service accepts 5 requests per second.\nA bucket with x tokens where a token is added to the bucket every 1/x seconds means the service accepts x requests per second.\nRequests that are received when the bucket is empty can just be dropped or enqueued to be handled later.\nImplementation in Rust The source code can be found here.\nThe bucket will accept x requests per second.\n#[derive(Debug)] struct Config {  /// The number of requests that can be accepted every second.  requests_per_second: usize, }  #[derive(Debug)] struct Bucket {  config: Config,  /// How many requests we can accept at this time.  tokens: AtomicUsize,  /// Sends are actually never made in this channel.  /// It is used only for the worker thread to know when the bucket  /// has been dropped and exit.  close_channel_sender: Sender\u0026lt;()\u0026gt;, } A thread is spawned to refill the bucket every 1/Config::requests_per_second, at this rate the bucket will accept around Config::requests_per_second requests per second.\nimpl Bucket {  pub fn new(config: Config) -\u0026gt; Arc\u0026lt;Self\u0026gt; {  let (sender, receiver) = crossbeam_channel::unbounded::\u0026lt;()\u0026gt;();   let tokens = AtomicUsize::new(1);   let bucket = Arc::new(Self {  config,  tokens,  close_channel_sender: sender,  });   let bucket_clone = Arc::downgrade(\u0026amp;bucket);  std::thread::spawn(move ||  Bucket::add_tokens_to_bucket_on_interval(  bucket_clone, receiver  )  );   bucket  }   fn add_tokens_to_bucket_on_interval(bucket: Weak\u0026lt;Bucket\u0026gt;, receiver: Receiver\u0026lt;()\u0026gt;) {  let interval = {  match bucket.upgrade() {  None =\u0026gt; {  error!(  \u0026#34;unable to define interval to add tokens to bucket because bucket has been dropped\u0026#34;  );  return;  }  Some(bucket) =\u0026gt;  Duration::from_secs_f64(1.0 / (bucket.config.requests_per_second as f64)),  }  };   debug!(?interval, \u0026#34;will add tokens to bucket at interval\u0026#34;);   let ticker = crossbeam_channel::tick(interval);   loop {  select! {  recv(ticker) -\u0026gt; _ =\u0026gt; match bucket.upgrade() {  None =\u0026gt; {  debug!(\u0026#34;cannot upgrade Weak ref to Arc, exiting\u0026#34;);  return;  }  Some(bucket) =\u0026gt; {  let _ = bucket  .tokens  .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |tokens| {  Some(std::cmp::min(tokens + 1, bucket.config.requests_per_second))  });  }  },  recv(receiver) -\u0026gt; message =\u0026gt; {  // An error is returned when we try to received from a  // channel that has been closed and this channel  // will only be closed when the bucket is dropped.  if message == Err(RecvError) {  debug!(\u0026#34; bucket has been dropped, won\u0026#39;t try to add tokens to the bucket anymore\u0026#34;  );  return;  }  }  }  }  } } And a function can be called to find out if there\u0026rsquo;s enough tokens in the bucket to accept a request. A token is consumed if the request is accepted.\nimpl Bucket {  ...   /// Returns true if there\u0026#39;s enough tokens in the bucket.  pub fn acquire(\u0026amp;self) -\u0026gt; bool {  self  .tokens  .fetch_update(Ordering::SeqCst, Ordering::SeqCst, |tokens| {  Some(if tokens \u0026gt; 0 { tokens - 1 } else { tokens })  })  .map(|tokens_in_the_bucket| tokens_in_the_bucket \u0026gt; 0)  .unwrap_or(false)  } } References https://en.wikipedia.org/wiki/Token_bucket\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/token_bucket/","summary":"Intro Token bucket is an algorithm that can be used to rate limit requests made or received by a service.\nHow it works The algorithm is called token bucket because of the way it works: imagine we have a bucket with x tokens where each accepted request consumes one token from the bucket and a token is added back to the bucket at an interval.\nA bucket with 1 token that is refilled each second means the service accepts one request per second.","title":"Token bucket"},{"content":"Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.\nThe authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus\n Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.\nRaft also includes a new mechanism for changing cluster membership.\nWhat is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.\nWhy Raft was created Paxos, another consensus algorithm, is the most popular algorithm when talking about consensus algorithms. Most implementations of consensus are based on Paxos or influenced by it.\nThe problem is that Paxos is hard to understand and its architecture does not support pratical systems without modifications.\nThe point of Raft is to be a consensus algorithm that is as efficient as Paxos but easier to understand and implement.\nWhat is unique about Raft Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov\u0026rsquo;s Viewstamped Replication), but it has several new features:\nStrong leader: Raft uses a stronger form of leadership than other consensus algorithms. For example, log entries only flow from the leader to other servers. This simplifies the management of the replicated log.\nLeader election: Raft uses randomized timers to elect leaders. This makes conflicts easier and faster to resolve.\nMembership changes: Raft\u0026rsquo;s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.\nRaft in the wild https://github.com/tikv/raft-rs\nhttps://github.com/hashicorp/raft\nhttps://github.com/sofastack/sofa-jraft\nhttps://github.com/eBay/NuRaft\nReplicated state machines States machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems.\nHow the consensus algorithms work with replicated state machines Replicated state machines are typically implemented using a replicated log.\nThe consensus algorithm manages a replicated log containg state machine commands from clients. The state machines process identical sequences of commands fro the logs, so they produce the same outputs.\nThe consensus algorithm module on the server receives commands from clients and adds them to its log. It communicates with the consensus algorithm modules on other servers to ensure that every log eventually contains the commands in the same order even if some of the servers fail.\nOnce commands are properly replicated (every server has the same commands in the same order), each server\u0026rsquo;s state machine processes them and the outputs are returned to the clients. As a result, the servers appear to form a single, highly reliable state machine.\nTypical properties of consensus algorithms Safety: They ensure that an incorrect result is never returned under all non-Byzantine conditions, including networks delays, partitions, and packet lost, duplication and reordering.\nAvailability: They are fully functional as the majority of the servers are functional. For example, a cluster of five servers can tolerate the failure of two servers. Servers that failed may rejoin the cluster after recovering.\nConsistency: They do not depend on timing to ensure the consistency of the logs.\nPerformance: A command can complete as soon as a majority of the cluster has responded has accepted the command.\nExamples of replicated state machines https://www.cs.cornell.edu/courses/cs6464/2009sp/lectures/16-chubby.pdf\nhttps://github.com/apache/zookeeper\nPaxos Created by Leslie Lamport, Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry. This is known as single-decree Paxos. Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log. This is known as multi-Paxos.\nPaxos criticism Paxos is difficult to understand. The full explanation is opaque and because of that a lot of effort is necessary to understand it and only a few people succeed in understanding it.\nBecause of its difficulty, there have been several attempts to simplify Paxos explanation. These explanation focus on the single-decree subset and even then, they are still hard to understand.\nThe second problem with Paxos is that there is no widely agreed-upon algorithm for multi-Paxos. Lamport\u0026rsquo;s descriptions are mostly about single-decree Paxos.\nThe third problem is that Paxos uses a peer-to-peer approach at its core. If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions.\nBecause of these problems, pratical systems implementations begin with Paxos, discover the difficulties in implementing, and then develop a significantly different architecture.\nThe Raft consensus algorithm Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. A leader can fail or become disconnected from the other servers, in which case a new leader is elected.\nRaft decomposes the consensus problem into three subproblems:\nLeader election: A new leader must be chosen when an existing leader fails.\nLog replication: The leader must accept log entries from clients and replicate them accross the cluster, forcing the other logs to agree with its own.\nSafety: If any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.\nRaft basics A raft cluster contains several servers, five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader, follower or candidate.\nleader: In normal operation there is exactly one leader and all of the other servers are followers. The leader handles all client requests.\nfollower: Followers simply respond to requests from leaders and candidates, if a client contacts a follower, the request is redirected to the leader.\ncandidate: A candidate is a follower that wants to become a leader.\nRaft divides time into terms of arbitraty length numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. If the election results in a split vote, the term ends with no leader and a new term with a new election begins shortly.\nEach server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate and if one server\u0026rsquo;s current term is smaller than the other\u0026rsquo;s, then it updates its current term to the larger value. If a candidate or leader discovers that is term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request.\nRaft servers communication with remote procedure calls Raft servers coomunicate using remote procedure calls. The basic consensus algorithm requires two types of RPCs:\nRequestVote: RequestVote RPCs are initiated by candidates during elections.\nAppendEntries: AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat.\nServers retry RPCs if they do not receive a response in a timely manner.\nLeader election Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers and remain in the follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader.\nTo begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each server in the cluster. A candidate continues in the candidate state until one of three things happens:\nWins election: The candidate wins the election and becomes the new leader.\nA candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server votes for at most one candidate in a given term, on a first-come-first-served basis. Once a candidate wins an election, it becomes the leader and sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.\nOther server wins election: Another candidate wins the election and becomes the new leader.\nDuring an election, a candidate may receive an AppendEntries RPC from another server claiming to be the leader. If the leader\u0026rsquo;s term is at least as large as the candidate\u0026rsquo;s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the request term is smaller than the candidate\u0026rsquo;s current term, the request is rejected as described in Raft basics.\nElection timeout: A period of time goes by with no winner.\nIf many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by increasing its term and iniating another round of RequestVote RPCs.\nElections timeouts are chosen randomly from the range like 150..300ms to ensure that split votes are rare and that hey are resolved quickly.\nEach candidate gets a randomized election timeout at the start of an election, and it waits for the timeout to elapse before starting a new election.\nElection restriction Raft uses the voting process to prevent a candidate from winning an election unless its log contains all commited entries from previous terms. When a RequestVote RPC is made, the candidate includes the index and term of the last entry in its log, the server that receives the request (aka the voter) denies the request if its own log is more up-to-date than of the candidate. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.\nLog replication Once a leader has been elected, it begins servicing client requests that each contain a command to be executed by the replicated state machines. When a request is received, the leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the servers to replicate the entry. Only after the entry has been replicated, the leader applies the entry to its state machine and returns the result of that execution to the client. The leader retries AppendEntries RPCs until all followers eventually store all log entries.\nLogs are composed of sequentially numbered entries. Each entry contains the term in which it was created and a command for the state machine. The leader decides when it is safe to apply a log entry to the state machines, entries that have been applied to the state machines are called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is commited once the leader that created the entry has replicated it on a majority of the servers. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs so that other server eventually find out. Once a follower learns that a log entry is committed, it applies every entry up to the entry to its local state machine.\n command term │ │ │ │ 0 1 2 3 4 5 6 7 log index │ │ ┌────┬────┬────┬────┬────┬────┬────┬────┐ │ ├─► 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ 3 │ │ └──────┴─►x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │ leader └────┴────┴────┴────┴────┴────┴────┴────┘ ┌────┬────┬────┬────┬────┐ │ 1 │ 1 │ 1 │ 2 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │ │ └────┴────┴────┴────┴────┘ │ │ ┌────┬────┬────┬────┬────┬────┬────┬────┐ │ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │ └────┴────┴────┴────┴────┴────┴────┴────┘ │ │ followers ┌────┬────┐ │ │ 1 │ 1 │ │ │x=3 │y=1 │ │ └────┴────┘ │ │ ┌────┬────┬────┬────┬────┬────┬────┐ │ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │ │ └────┴────┴────┴────┴────┴────┴────┘ │ │ └──────────────────────────────────┘ committed entries  Raft mantains the following properties:\n  If two entries in different logs have the same index and term, then they store the same command.\n  If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\n  How log inconsistencies are handled TLDR: Leader overwrites follower logs if they are out of sync.\nIn Raft, the leader handles inconsistencies by forcing the follower\u0026rsquo;s logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader\u0026rsquo;s log. To bring a follower\u0026rsquo;s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower\u0026rsquo;s log after that point, and send the follower all of the leader\u0026rsquo;s entries after that point. The leader maintains a nextIndex for each follower, which is the index of the next log the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log.\nSafety TLDR: Only servers that contain all of the entries commited in previous terms may become leader at any given term.\nThere\u0026rsquo;s a problem with the Raft description so far:\nFor example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones. As a result, different state machines might execute different command sequences. This is fixed by adding a restriction on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries commited in previous terms.\nRestriction on committing logs Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader\u0026rsquo;s current term are commited by counting replicas.\nFollower and candidate crashes If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely, if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCS are idempotent, so this causes no harm. If a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.\nTiming and availability Raft will be able to elect and maintain a steady leader as long as the system satifies the following timing requirement:\nbroadcast_time \u0026lt;= election_timeout \u0026lt;= MTBF\nbroad_cast_time is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses.\nelection_timeout is the election timeout described in Leader election.\nMTBF is the average time between failures for a single server.\nThe broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbet messages required to keep followers from starting elections.\nThe election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.\nCluster membership changes Configuration changes are incorporated into the Raft consensus algorithm. In Raft the cluster first switches to a transitional configuration called joint consensus, once the joint consensus has been committed, the system then transitions to the new configuration.\nThe join consensus combines both the old an new configurations:\n Log entries are replicated to all servers in both configurations. Any server from either configuration may serve as leader. Agreement for elections and entry commitment requires separate majorities from both the old and new configurations.  The joint consensus allows individual servers to transition between configurations at different times without compromising safety.\nHow cluster configurations are stored and communicated Cluster configurations are stored and communicated using special entries in the replicated log. When the leader receives a request to change the configuration from config_old (the current configuration) to config_new (the new configuration), it stores a tuple (config_old, config_new) as a log entry and replicates that entry. Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions even if the entry has not been committed yet.\nLog compaction TLDR: We don\u0026rsquo;t have infinite memory, discard logs that aren\u0026rsquo;t needed anymore.\nRaft\u0026rsquo;s log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound. As the log grows longer, it occupies more spaces and takes more time to replay.\nSnapshotting is the simplest approach to compaction. In snapshotting the entire current system state is written to a snapshot on stable storage, then the tire log up to that point is discarded. Snapshotting is used in Chubby and ZooKeeper and in Raft as well.\nThe basic idea of snapshotting in Raft: TLDR: Add the current machine state to the log and delete all logs used to get to this state. The snapshot should also be written to stable storage.\nEach server takes snapshots independently, covering just the commited entries in its log. Most of the work consists of the state machine writing its current state to the snapshot. Raft also includes a small amount of metadata in the snapshot: the last included index which is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied), and the last included term which is the term of the entry. The snapshot also includes the latest configuration. Once a server completes a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.\n 0 1 2 3 4 5 6 log index ┌────┬────┬────┬────┬────┬────┬────┐ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ │ before snapshot │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │ │ └────┴────┴────┴────┴────┴────┴────┘ snapshot ┌────────────────────────┬────┬────┐ │last included index: 4 │ 3 │ 3 │ │ │last included term: 3 │y=7 │x=5 │ │ │state machine state: ├────┴────┘ │ after snapshot │ x = 0 │ │ │ y = 9 │ │ └────────────────────────┘ │ │ └────────────────────────┘ committed entries  When a new follower joins the cluster The way to bring a new foller up-to-date is for the leader to send it a snapshot over the network. The leader uses a new RPC called InstallSnapShot to send snapshots to followers that are too far behind because they are new or because they are too slow.\nWhen a follower receives a snapshot with this RPC:\nIf the snapshot contains new information on in the follower\u0026rsquo;s log, the follower discards and replaces its log with the snapshot.\nIf the snapshot contains only a prefix of its log, then log entries covered by the snapshot are deleted bu entries following the snapshot are still valid and must be retained.\nClient interaction Clients of Raft send all of their requests to the leader.\nHow clients find the leader When a client starts up, it connects to a randomly-chosen server. If the client\u0026rsquo;s choice is not the leader, that server will reject the client\u0026rsquo;s request and supply information about the most recent leader it has heard from.\nCommand deduplication For example, if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing to be executed a second time.\nThe solution is for clients to assign unique serial numbers to every command. Then, the state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number ha already been executed, it responses immediatelly without re-executing the request.\nRead-only operations should not return stale data When a leader responds to a request, its log could be outdated with a new leader had been elected in the meantime.\nTo avoid this situation:\nA leader must have the latest information on which entries are committed. Because of that, each leader commits a blank no-op entry into the log at the start of its term.\nA leader must check whether it has been deposed before processing a read-only request. Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only request.\nReferences In Search of an Understandable Consensus Algorithm (Extended Version) - https://raft.github.io/raft.pdf\nDesigning for Understandability: The Raft Consensus Algorithm - https://www.youtube.com/watch?v=vYp4LYbnnW8 \u0026ldquo;Raft - The Understandable Distributed Protocol\u0026rdquo; by Ben Johnson (2013) - https://www.youtube.com/watch?v=ro2fU8_mr2w\nhttps://github.com/hashicorp/raft\nMIT 6.824: Distributed Systems (Spring 2020) Lecture 6: Fault Tolerance: Raft - https://www.youtube.com/watch?v=64Zp3tzNbpE\u0026amp;list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB\u0026amp;index=6\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/raft_notes/","summary":"Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.\nThe authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus\n Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.\nRaft also includes a new mechanism for changing cluster membership.\nWhat is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.","title":"Notes taken from the Raft paper"},{"content":"What\u0026rsquo;s a Bloom filter A bloom filter is a data-structure that can be used to check if a set contains an element. It uses way less memory than a conventional set data-structure by sacrificing accuracy.\nExample Say we are building a log-structured merge-tree, we can use a bloom filter to find out if the LSM-tree contains a particular key in O(1) time in most cases, the downside is that sometimes the bloom filter would say that the LSM-tree contains a key, but it actually does not and we would go searching for the value that\u0026rsquo;s mapped to the key and never actually find it.\nIt is used in a lot of places.\nHow it works A bloom filter is just a bit-set that uses n deterministic hash functions to add elements to it.\nempty bit-set\nAdding elements to the set To add the key bob to the set, we run the key through each of the n hash functions and map the hash function output to one of the positions in the bit-set and for each position, we flip the bit to 1.\nbit-set after bob was added to the bloom filter Finding out if the set contains an element To find out if the set contains the key bob, we run the key through each of the n hash functions again \u0026ndash; since the hash functions must be deterministic they will always map to the same position in the bit-set \u0026ndash; and check if the bit is set to 1 for each of the bit-set positions we reached after running the key through the hash functions. If every hash function maps to a bit set to 1, it means the key is in the set.\nbob is in the set because every hash function mapped it to a bit set to 1 alice is not in the set because not every hash function mapped to a bit set to 1 False positives Since collisions can happen some keys will be mapped to bits that were set to 1 when other keys were added to the set. In this case, the bloom filter will say that it contains the key even though it does not.\nthe bit-set after bob was added to it since john maps to the same bits as bob and the bits were set to 1 after bob was added to the set, we got a false positive Removing an element from the set As it stands, removing an element from the set is not actually possible. If we had a bloom filter that uses 3 hash functions that looks like this after adding alice and bob to it:\nbloom filter after adding alice and bob to it with 3 hash functions Note that alice and bob hash to the same position in the bit-set for some of the hash functions\nbits shared between alice and bob are in white The naive solution is to remove alice from the bloom filter by setting the bits mapped by hashi(alice) to 0:\nbits that were flipped to 0 are in white Now, let\u0026rsquo;s check if alice is in the set, for a key to be in the set hashi(key) must map to bits set to 1\n hashi(alice) maps to bits set to 0 which means alice is not in the set alice is not in the set as expected. Let\u0026rsquo;s see if bob is still in the set, it should be since we didn\u0026rsquo;t remove it.\nnot every hashi(bob) maps to bits set to 1 which means bob is not in the set, bits set to 0 after removing alice from the set are in white bob is not in the set anymore, even though we didn\u0026rsquo;t remove it. The problem is that since keys may share the positions in the bit-set, we cannot just flip bits back to 0 to remove a key from the set because in doing so we may flip bits that are used by other keys.\nCounting bloom filter Since we cannot flip bits back to 0 to remove a key from the set, we could maintain a counter instead of a single bit. When a key is added to the set, the counter is incremented and when a key is removed from the set, the counter is decremented. If the counter reaches 0, it means no keys are mapped to the position.\nPositions that have more than one key mapped to it will have a counter greater than 1.\npositions in white are shared between two or more keys and have a counter greater than 1 removing alice from the set by decrementing the counters mapped by hashi(alice) After decrementing the counters, not every hashi(alice) maps to a counter greater than 0 which means alice is not in the set anymore. Unlike the bloom filter that uses only bits, hashi(bob) still maps to counters that are greater than 0 which means bob is still in the set.\nExample in Rust https://github.com/PoorlyDefinedBehaviour/bloom_filter/\nReferences Network Applications of Bloom Filters: A Survey - https://www.eecs.harvard.edu/~michaelm/postscripts/im2005b.pdf\nDesigning Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems - Martin Kleppmann\n","permalink":"https://poorlydefinedbehaviour.github.io/posts/bloom_filter/","summary":"What\u0026rsquo;s a Bloom filter A bloom filter is a data-structure that can be used to check if a set contains an element. It uses way less memory than a conventional set data-structure by sacrificing accuracy.\nExample Say we are building a log-structured merge-tree, we can use a bloom filter to find out if the LSM-tree contains a particular key in O(1) time in most cases, the downside is that sometimes the bloom filter would say that the LSM-tree contains a key, but it actually does not and we would go searching for the value that\u0026rsquo;s mapped to the key and never actually find it.","title":"Bloom filter"}]