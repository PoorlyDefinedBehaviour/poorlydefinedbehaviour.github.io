<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes taken from the Raft paper |</title><meta name=keywords content><meta name=description content="Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.
The authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus
 Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.
Raft also includes a new mechanism for changing cluster membership.
What is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members."><meta name=author content="poorlydefinedbehaviour"><link rel=canonical href=https://poorlydefinedbehaviour.github.io/posts/raft_notes/><link crossorigin=anonymous href=/assets/css/stylesheet.min.48a18943c2fc15c38a372b8dde1f5e5dc0bc64fa6cb90f5a817d2f8c76b7f3ae.css integrity="sha256-SKGJQ8L8FcOKNyuN3h9eXcC8ZPpsuQ9agX0vjHa3864=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://poorlydefinedbehaviour.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://poorlydefinedbehaviour.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://poorlydefinedbehaviour.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://poorlydefinedbehaviour.github.io/apple-touch-icon.png><link rel=mask-icon href=https://poorlydefinedbehaviour.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Notes taken from the Raft paper"><meta property="og:description" content="Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.
The authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus
 Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.
Raft also includes a new mechanism for changing cluster membership.
What is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members."><meta property="og:type" content="article"><meta property="og:url" content="https://poorlydefinedbehaviour.github.io/posts/raft_notes/"><meta property="og:image" content="https://poorlydefinedbehaviour.github.io/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-04T17:48:19-03:00"><meta property="article:modified_time" content="2022-03-04T17:48:19-03:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://poorlydefinedbehaviour.github.io/papermod-cover.png"><meta name=twitter:title content="Notes taken from the Raft paper"><meta name=twitter:description content="Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.
The authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus
 Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.
Raft also includes a new mechanism for changing cluster membership.
What is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://poorlydefinedbehaviour.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Notes taken from the Raft paper","item":"https://poorlydefinedbehaviour.github.io/posts/raft_notes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes taken from the Raft paper","name":"Notes taken from the Raft paper","description":"Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.\nThe authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus\n Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.\nRaft also includes a new mechanism for changing cluster membership.\nWhat is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.","keywords":[],"articleBody":"Replicated And Fault Tolerant Raft is a consensus algorithm for managing a replicated log.\nThe authors claim Raft to be more understandable than Paxos because Raft separates the key elements of consensus\n Leader election Log replication Safety  and enforces a stronger degree of coherency to reduce the number of states that must be considered.\nRaft also includes a new mechanism for changing cluster membership.\nWhat is a consensus algorithm Consensus algorithms allow a collection of machines to work as a coherent group that can survive the failures of some of its members.\nWhy Raft was created Paxos, another consensus algorithm, is the most popular algorithm when talking about consensus algorithms. Most implementations of consensus are based on Paxos or influenced by it.\nThe problem is that Paxos is hard to understand and its architecture does not support pratical systems without modifications.\nThe point of Raft is to be a consensus algorithm that is as efficient as Paxos but easier to understand and implement.\nWhat is unique about Raft Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov’s Viewstamped Replication), but it has several new features:\nStrong leader: Raft uses a stronger form of leadership than other consensus algorithms. For example, log entries only flow from the leader to other servers. This simplifies the management of the replicated log.\nLeader election: Raft uses randomized timers to elect leaders. This makes conflicts easier and faster to resolve.\nMembership changes: Raft’s mechanism for changing the set of servers in the cluster uses a new joint consensus approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.\nRaft in the wild https://github.com/tikv/raft-rs\nhttps://github.com/hashicorp/raft\nhttps://github.com/sofastack/sofa-jraft\nhttps://github.com/eBay/NuRaft\nReplicated state machines States machines on a collection of servers compute identical copies of the same state and can continue operating even if some of the servers are down. Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems.\nHow the consensus algorithms work with replicated state machines Replicated state machines are typically implemented using a replicated log.\nThe consensus algorithm manages a replicated log containg state machine commands from clients. The state machines process identical sequences of commands fro the logs, so they produce the same outputs.\nThe consensus algorithm module on the server receives commands from clients and adds them to its log. It communicates with the consensus algorithm modules on other servers to ensure that every log eventually contains the commands in the same order even if some of the servers fail.\nOnce commands are properly replicated (every server has the same commands in the same order), each server’s state machine processes them and the outputs are returned to the clients. As a result, the servers appear to form a single, highly reliable state machine.\nTypical properties of consensus algorithms Safety: They ensure that an incorrect result is never returned under all non-Byzantine conditions, including networks delays, partitions, and packet lost, duplication and reordering.\nAvailability: They are fully functional as the majority of the servers are functional. For example, a cluster of five servers can tolerate the failure of two servers. Servers that failed may rejoin the cluster after recovering.\nConsistency: They do not depend on timing to ensure the consistency of the logs.\nPerformance: A command can complete as soon as a majority of the cluster has responded has accepted the command.\nExamples of replicated state machines https://www.cs.cornell.edu/courses/cs6464/2009sp/lectures/16-chubby.pdf\nhttps://github.com/apache/zookeeper\nPaxos Created by Leslie Lamport, Paxos first defines a protocol capable of reaching agreement on a single decision, such as a single replicated log entry. This is known as single-decree Paxos. Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log. This is known as multi-Paxos.\nPaxos criticism Paxos is difficult to understand. The full explanation is opaque and because of that a lot of effort is necessary to understand it and only a few people succeed in understanding it.\nBecause of its difficulty, there have been several attempts to simplify Paxos explanation. These explanation focus on the single-decree subset and even then, they are still hard to understand.\nThe second problem with Paxos is that there is no widely agreed-upon algorithm for multi-Paxos. Lamport’s descriptions are mostly about single-decree Paxos.\nThe third problem is that Paxos uses a peer-to-peer approach at its core. If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions.\nBecause of these problems, pratical systems implementations begin with Paxos, discover the difficulties in implementing, and then develop a significantly different architecture.\nThe Raft consensus algorithm Raft implements consensus by first electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log. The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines. A leader can fail or become disconnected from the other servers, in which case a new leader is elected.\nRaft decomposes the consensus problem into three subproblems:\nLeader election: A new leader must be chosen when an existing leader fails.\nLog replication: The leader must accept log entries from clients and replicate them accross the cluster, forcing the other logs to agree with its own.\nSafety: If any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.\nRaft basics A raft cluster contains several servers, five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: leader, follower or candidate.\nleader: In normal operation there is exactly one leader and all of the other servers are followers. The leader handles all client requests.\nfollower: Followers simply respond to requests from leaders and candidates, if a client contacts a follower, the request is redirected to the leader.\ncandidate: A candidate is a follower that wants to become a leader.\nRaft divides time into terms of arbitraty length numbered with consecutive integers. Each term begins with an election, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. If the election results in a split vote, the term ends with no leader and a new term with a new election begins shortly.\nEach server stores a current term number, which increases monotonically over time. Current terms are exchanged whenever servers communicate and if one server’s current term is smaller than the other’s, then it updates its current term to the larger value. If a candidate or leader discovers that is term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request.\nRaft servers communication with remote procedure calls Raft servers coomunicate using remote procedure calls. The basic consensus algorithm requires two types of RPCs:\nRequestVote: RequestVote RPCs are initiated by candidates during elections.\nAppendEntries: AppendEntries RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat.\nServers retry RPCs if they do not receive a response in a timely manner.\nLeader election Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers and remain in the follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and begins an election to choose a new leader.\nTo begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues RequestVote RPCs in parallel to each server in the cluster. A candidate continues in the candidate state until one of three things happens:\nWins election: The candidate wins the election and becomes the new leader.\nA candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server votes for at most one candidate in a given term, on a first-come-first-served basis. Once a candidate wins an election, it becomes the leader and sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.\nOther server wins election: Another candidate wins the election and becomes the new leader.\nDuring an election, a candidate may receive an AppendEntries RPC from another server claiming to be the leader. If the leader’s term is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate and returns to follower state. If the request term is smaller than the candidate’s current term, the request is rejected as described in Raft basics.\nElection timeout: A period of time goes by with no winner.\nIf many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by increasing its term and iniating another round of RequestVote RPCs.\nElections timeouts are chosen randomly from the range like 150..300ms to ensure that split votes are rare and that hey are resolved quickly.\nEach candidate gets a randomized election timeout at the start of an election, and it waits for the timeout to elapse before starting a new election.\nElection restriction Raft uses the voting process to prevent a candidate from winning an election unless its log contains all commited entries from previous terms. When a RequestVote RPC is made, the candidate includes the index and term of the last entry in its log, the server that receives the request (aka the voter) denies the request if its own log is more up-to-date than of the candidate. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.\nLog replication Once a leader has been elected, it begins servicing client requests that each contain a command to be executed by the replicated state machines. When a request is received, the leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the servers to replicate the entry. Only after the entry has been replicated, the leader applies the entry to its state machine and returns the result of that execution to the client. The leader retries AppendEntries RPCs until all followers eventually store all log entries.\nLogs are composed of sequentially numbered entries. Each entry contains the term in which it was created and a command for the state machine. The leader decides when it is safe to apply a log entry to the state machines, entries that have been applied to the state machines are called committed. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is commited once the leader that created the entry has replicated it on a majority of the servers. The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs so that other server eventually find out. Once a follower learns that a log entry is committed, it applies every entry up to the entry to its local state machine.\n command term │ │ │ │ 0 1 2 3 4 5 6 7 log index │ │ ┌────┬────┬────┬────┬────┬────┬────┬────┐ │ ├─► 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ 3 │ │ └──────┴─►x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │ leader └────┴────┴────┴────┴────┴────┴────┴────┘ ┌────┬────┬────┬────┬────┐ │ 1 │ 1 │ 1 │ 2 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │ │ └────┴────┴────┴────┴────┘ │ │ ┌────┬────┬────┬────┬────┬────┬────┬────┐ │ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │ └────┴────┴────┴────┴────┴────┴────┴────┘ │ │ followers ┌────┬────┐ │ │ 1 │ 1 │ │ │x=3 │y=1 │ │ └────┴────┘ │ │ ┌────┬────┬────┬────┬────┬────┬────┐ │ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ │ │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │ │ └────┴────┴────┴────┴────┴────┴────┘ │ │ └──────────────────────────────────┘ committed entries  Raft mantains the following properties:\n  If two entries in different logs have the same index and term, then they store the same command.\n  If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.\n  How log inconsistencies are handled TLDR: Leader overwrites follower logs if they are out of sync.\nIn Raft, the leader handles inconsistencies by forcing the follower’s logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader’s log. To bring a follower’s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point. The leader maintains a nextIndex for each follower, which is the index of the next log the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log.\nSafety TLDR: Only servers that contain all of the entries commited in previous terms may become leader at any given term.\nThere’s a problem with the Raft description so far:\nFor example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones. As a result, different state machines might execute different command sequences. This is fixed by adding a restriction on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries commited in previous terms.\nRestriction on committing logs Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader’s current term are commited by counting replicas.\nFollower and candidate crashes If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely, if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCS are idempotent, so this causes no harm. If a follower receives an AppendEntries request that includes log entries already present in its log, it ignores those entries in the new request.\nTiming and availability Raft will be able to elect and maintain a steady leader as long as the system satifies the following timing requirement:\nbroadcast_time broad_cast_time is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses.\nelection_timeout is the election timeout described in Leader election.\nMTBF is the average time between failures for a single server.\nThe broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbet messages required to keep followers from starting elections.\nThe election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.\nCluster membership changes Configuration changes are incorporated into the Raft consensus algorithm. In Raft the cluster first switches to a transitional configuration called joint consensus, once the joint consensus has been committed, the system then transitions to the new configuration.\nThe join consensus combines both the old an new configurations:\n Log entries are replicated to all servers in both configurations. Any server from either configuration may serve as leader. Agreement for elections and entry commitment requires separate majorities from both the old and new configurations.  The joint consensus allows individual servers to transition between configurations at different times without compromising safety.\nHow cluster configurations are stored and communicated Cluster configurations are stored and communicated using special entries in the replicated log. When the leader receives a request to change the configuration from config_old (the current configuration) to config_new (the new configuration), it stores a tuple (config_old, config_new) as a log entry and replicates that entry. Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions even if the entry has not been committed yet.\nLog compaction TLDR: We don’t have infinite memory, discard logs that aren’t needed anymore.\nRaft’s log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound. As the log grows longer, it occupies more spaces and takes more time to replay.\nSnapshotting is the simplest approach to compaction. In snapshotting the entire current system state is written to a snapshot on stable storage, then the tire log up to that point is discarded. Snapshotting is used in Chubby and ZooKeeper and in Raft as well.\nThe basic idea of snapshotting in Raft: TLDR: Add the current machine state to the log and delete all logs used to get to this state. The snapshot should also be written to stable storage.\nEach server takes snapshots independently, covering just the commited entries in its log. Most of the work consists of the state machine writing its current state to the snapshot. Raft also includes a small amount of metadata in the snapshot: the last included index which is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied), and the last included term which is the term of the entry. The snapshot also includes the latest configuration. Once a server completes a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.\n 0 1 2 3 4 5 6 log index ┌────┬────┬────┬────┬────┬────┬────┐ │ 1 │ 1 │ 1 │ 2 │ 3 │ 3 │ 3 │ │ before snapshot │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │ │ └────┴────┴────┴────┴────┴────┴────┘ snapshot ┌────────────────────────┬────┬────┐ │last included index: 4 │ 3 │ 3 │ │ │last included term: 3 │y=7 │x=5 │ │ │state machine state: ├────┴────┘ │ after snapshot │ x = 0 │ │ │ y = 9 │ │ └────────────────────────┘ │ │ └────────────────────────┘ committed entries  When a new follower joins the cluster The way to bring a new foller up-to-date is for the leader to send it a snapshot over the network. The leader uses a new RPC called InstallSnapShot to send snapshots to followers that are too far behind because they are new or because they are too slow.\nWhen a follower receives a snapshot with this RPC:\nIf the snapshot contains new information on in the follower’s log, the follower discards and replaces its log with the snapshot.\nIf the snapshot contains only a prefix of its log, then log entries covered by the snapshot are deleted bu entries following the snapshot are still valid and must be retained.\nClient interaction Clients of Raft send all of their requests to the leader.\nHow clients find the leader When a client starts up, it connects to a randomly-chosen server. If the client’s choice is not the leader, that server will reject the client’s request and supply information about the most recent leader it has heard from.\nCommand deduplication For example, if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing to be executed a second time.\nThe solution is for clients to assign unique serial numbers to every command. Then, the state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number ha already been executed, it responses immediatelly without re-executing the request.\nRead-only operations should not return stale data When a leader responds to a request, its log could be outdated with a new leader had been elected in the meantime.\nTo avoid this situation:\nA leader must have the latest information on which entries are committed. Because of that, each leader commits a blank no-op entry into the log at the start of its term.\nA leader must check whether it has been deposed before processing a read-only request. Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only request.\nReferences In Search of an Understandable Consensus Algorithm (Extended Version) - https://raft.github.io/raft.pdf\nDesigning for Understandability: The Raft Consensus Algorithm - https://www.youtube.com/watch?v=vYp4LYbnnW8 “Raft - The Understandable Distributed Protocol” by Ben Johnson (2013) - https://www.youtube.com/watch?v=ro2fU8_mr2w\nhttps://github.com/hashicorp/raft\nMIT 6.824: Distributed Systems (Spring 2020) Lecture 6: Fault Tolerance: Raft - https://www.youtube.com/watch?v=64Zp3tzNbpE\u0026list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB\u0026index=6\n","wordCount":"3557","inLanguage":"en","datePublished":"2022-03-04T17:48:19-03:00","dateModified":"2022-03-04T17:48:19-03:00","author":{"@type":"Person","name":"poorlydefinedbehaviour"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://poorlydefinedbehaviour.github.io/posts/raft_notes/"},"publisher":{"@type":"Organization","name":"","logo":{"@type":"ImageObject","url":"https://poorlydefinedbehaviour.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://poorlydefinedbehaviour.github.io/ title=Home><span>Home</span></a></li><li><a href=https://poorlydefinedbehaviour.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://poorlydefinedbehaviour.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://poorlydefinedbehaviour.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://poorlydefinedbehaviour.github.io>Home</a>&nbsp;»&nbsp;<a href=https://poorlydefinedbehaviour.github.io/posts/>Posts</a></div><h1 class=post-title>Notes taken from the Raft paper</h1><div class=post-meta><span title="2022-03-04 17:48:19 -0300 -0300">March 4, 2022</span>&nbsp;·&nbsp;17 min&nbsp;·&nbsp;poorlydefinedbehaviour&nbsp;|&nbsp;<a href=https://github.com/PoorlyDefinedBehaviour/poorlydefinedbehaviour.github.io/tree/main/content/posts/raft_notes.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#replicated-and-fault-tolerant aria-label="Replicated And Fault Tolerant"><strong>R</strong>eplicated <strong>A</strong>nd <strong>F</strong>ault <strong>T</strong>olerant</a><ul><li><a href=#what-is-a-consensus-algorithm aria-label="What is a consensus algorithm">What is a consensus algorithm</a></li><li><a href=#why-raft-was-created aria-label="Why Raft was created">Why Raft was created</a></li><li><a href=#what-is-unique-about-raft aria-label="What is unique about Raft">What is unique about Raft</a></li><li><a href=#raft-in-the-wild aria-label="Raft in the wild">Raft in the wild</a></li></ul></li><li><a href=#replicated-state-machines aria-label="Replicated state machines">Replicated state machines</a><ul><li><a href=#how-the-consensus-algorithms-work-with-replicated-state-machines aria-label="How the consensus algorithms work with replicated state machines">How the consensus algorithms work with replicated state machines</a></li><li><a href=#typical-properties-of-consensus-algorithms aria-label="Typical properties of consensus algorithms">Typical properties of consensus algorithms</a></li><li><a href=#examples-of-replicated-state-machines aria-label="Examples of replicated state machines">Examples of replicated state machines</a></li></ul></li><li><a href=#paxos aria-label=Paxos>Paxos</a><ul><li><a href=#paxos-criticism aria-label="Paxos criticism">Paxos criticism</a></li></ul></li><li><a href=#the-raft-consensus-algorithm aria-label="The Raft consensus algorithm">The Raft consensus algorithm</a><ul><li><a href=#raft-basics aria-label="Raft basics">Raft basics</a></li><li><a href=#raft-servers-communication-with-remote-procedure-calls aria-label="Raft servers communication with remote procedure calls">Raft servers communication with remote procedure calls</a></li><li><a href=#leader-election aria-label="Leader election">Leader election</a></li><li><a href=#election-restriction aria-label="Election restriction">Election restriction</a></li><li><a href=#log-replication aria-label="Log replication">Log replication</a></li><li><a href=#how-log-inconsistencies-are-handled aria-label="How log inconsistencies are handled">How log inconsistencies are handled</a></li></ul></li><li><a href=#safety aria-label=Safety>Safety</a><ul><li><a href=#restriction-on-committing-logs aria-label="Restriction on committing logs">Restriction on committing logs</a></li></ul></li><li><a href=#follower-and-candidate-crashes aria-label="Follower and candidate crashes">Follower and candidate crashes</a></li><li><a href=#timing-and-availability aria-label="Timing and availability">Timing and availability</a></li><li><a href=#cluster-membership-changes aria-label="Cluster membership changes">Cluster membership changes</a><ul><li><a href=#how-cluster-configurations-are-stored-and-communicated aria-label="How cluster configurations are stored and communicated">How cluster configurations are stored and communicated</a></li></ul></li><li><a href=#log-compaction aria-label="Log compaction">Log compaction</a><ul><li><a href=#the-basic-idea-of-snapshotting-in-raft aria-label="The basic idea of snapshotting in Raft:">The basic idea of snapshotting in Raft:</a></li><li><a href=#when-a-new-follower-joins-the-cluster aria-label="When a new follower joins the cluster">When a new follower joins the cluster</a></li></ul></li><li><a href=#client-interaction aria-label="Client interaction">Client interaction</a><ul><li><a href=#how-clients-find-the-leader aria-label="How clients find the leader">How clients find the leader</a></li><li><a href=#command-deduplication aria-label="Command deduplication">Command deduplication</a></li><li><a href=#read-only-operations-should-not-return-stale-data aria-label="Read-only operations should not return stale data">Read-only operations should not return stale data</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h1 id=replicated-and-fault-tolerant><strong>R</strong>eplicated <strong>A</strong>nd <strong>F</strong>ault <strong>T</strong>olerant<a hidden class=anchor aria-hidden=true href=#replicated-and-fault-tolerant>#</a></h1><p>Raft is a consensus algorithm for managing a replicated log.</p><p>The authors claim Raft to be more understandable than Paxos
because Raft separates the key elements of consensus</p><ul><li>Leader election</li><li>Log replication</li><li>Safety</li></ul><p>and enforces a stronger degree of coherency to reduce the number of states
that must be considered.</p><p>Raft also includes a new mechanism for changing cluster membership.</p><h2 id=what-is-a-consensus-algorithm>What is a consensus algorithm<a hidden class=anchor aria-hidden=true href=#what-is-a-consensus-algorithm>#</a></h2><p>Consensus algorithms allow a collection of machines to work as a coherent group
that can survive the failures of some of its members.</p><h2 id=why-raft-was-created>Why Raft was created<a hidden class=anchor aria-hidden=true href=#why-raft-was-created>#</a></h2><p>Paxos, another consensus algorithm, is the most popular algorithm when talking
about consensus algorithms. Most implementations of consensus are based on Paxos
or influenced by it.</p><p>The problem is that Paxos is hard to understand and its architecture does not support pratical systems without modifications.</p><p>The point of Raft is to be a consensus algorithm that is as efficient as Paxos
but easier to understand and implement.</p><h2 id=what-is-unique-about-raft>What is unique about Raft<a hidden class=anchor aria-hidden=true href=#what-is-unique-about-raft>#</a></h2><p>Raft is similar in many ways to existing consensus algorithms (most notably, Oki and Liskov&rsquo;s Viewstamped Replication), but it has several new features:</p><p><strong>Strong leader</strong>: Raft uses a stronger form of leadership than other consensus algorithms. For example, log entries only flow from the leader to other servers.
This simplifies the management of the replicated log.</p><p><strong>Leader election</strong>: Raft uses randomized timers to elect leaders. This makes conflicts easier and faster to resolve.</p><p><strong>Membership changes</strong>: Raft&rsquo;s mechanism for changing the set of servers in the cluster uses a new <em>joint consensus</em> approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.</p><h2 id=raft-in-the-wild>Raft in the wild<a hidden class=anchor aria-hidden=true href=#raft-in-the-wild>#</a></h2><p><a href=https://github.com/tikv/raft-rs>https://github.com/tikv/raft-rs</a><br><a href=https://github.com/hashicorp/raft>https://github.com/hashicorp/raft</a><br><a href=https://github.com/sofastack/sofa-jraft>https://github.com/sofastack/sofa-jraft</a><br><a href=https://github.com/eBay/NuRaft>https://github.com/eBay/NuRaft</a></p><h1 id=replicated-state-machines>Replicated state machines<a hidden class=anchor aria-hidden=true href=#replicated-state-machines>#</a></h1><p>States machines on a collection of servers compute identical copies of the same
state and can continue operating even if some of the servers are down.
Replicated state machines are used to solve a variety of fault tolerance problems in distributed systems.</p><h2 id=how-the-consensus-algorithms-work-with-replicated-state-machines>How the consensus algorithms work with replicated state machines<a hidden class=anchor aria-hidden=true href=#how-the-consensus-algorithms-work-with-replicated-state-machines>#</a></h2><p>Replicated state machines are typically implemented using a replicated log.</p><p>The consensus algorithm manages a replicated log containg state machine
commands from clients. The state machines process identical sequences of commands
fro the logs, so they produce the same outputs.</p><p>The consensus algorithm module on the server receives commands from clients
and adds them to its log. It communicates with the consensus algorithm modules on other servers to ensure that every log eventually contains the commands in the same order even if some of the servers fail.</p><p>Once commands are properly replicated (every server has the same commands in the same order), each server&rsquo;s state machine processes them and the outputs are returned to the clients. As a result, the servers appear to form a single, highly
reliable state machine.</p><h2 id=typical-properties-of-consensus-algorithms>Typical properties of consensus algorithms<a hidden class=anchor aria-hidden=true href=#typical-properties-of-consensus-algorithms>#</a></h2><p><strong>Safety</strong>: They ensure that an incorrect result is never returned under all non-<a href=https://en.wikipedia.org/wiki/Byzantine_fault>Byzantine conditions</a>, including networks delays, partitions, and packet lost, duplication and reordering.</p><p><strong>Availability</strong>: They are fully functional as the majority of the servers are functional. For example, a cluster of five servers can tolerate the failure of two servers. Servers that failed may rejoin the cluster after recovering.</p><p><strong>Consistency</strong>: They do not depend on timing to ensure the consistency of the logs.</p><p><strong>Performance</strong>: A command can complete as soon as a majority of the cluster has responded has accepted the command.</p><h2 id=examples-of-replicated-state-machines>Examples of replicated state machines<a hidden class=anchor aria-hidden=true href=#examples-of-replicated-state-machines>#</a></h2><p><a href=https://www.cs.cornell.edu/courses/cs6464/2009sp/lectures/16-chubby.pdf>https://www.cs.cornell.edu/courses/cs6464/2009sp/lectures/16-chubby.pdf</a><br><a href=https://github.com/apache/zookeeper>https://github.com/apache/zookeeper</a></p><h1 id=paxos>Paxos<a hidden class=anchor aria-hidden=true href=#paxos>#</a></h1><p>Created by Leslie Lamport, Paxos first defines a protocol capable of reaching
agreement on a single decision, such as a single replicated log entry. This is known as single-decree Paxos. Paxos then combines multiple instances of this protocol to facilitate a series of decisions such as a log. This is known as multi-Paxos.</p><h2 id=paxos-criticism>Paxos criticism<a hidden class=anchor aria-hidden=true href=#paxos-criticism>#</a></h2><p>Paxos is difficult to understand. The full explanation is opaque and because of that a lot of effort is necessary to understand it and only a few people succeed in understanding it.</p><p>Because of its difficulty, there have been several attempts to simplify Paxos explanation. These explanation focus on the single-decree subset and even then, they are still hard to understand.</p><p>The second problem with Paxos is that there is no widely agreed-upon algorithm for multi-Paxos. Lamport&rsquo;s descriptions are mostly about single-decree Paxos.</p><p>The third problem is that Paxos uses a peer-to-peer approach at its core. If a series of decisions must be made, it is simpler and faster to first elect a leader, then have the leader coordinate the decisions.</p><p>Because of these problems, pratical systems implementations begin with Paxos, discover the difficulties in implementing, and then develop a significantly different architecture.</p><h1 id=the-raft-consensus-algorithm>The Raft consensus algorithm<a hidden class=anchor aria-hidden=true href=#the-raft-consensus-algorithm>#</a></h1><p>Raft implements consensus by first electing a distinguished <strong>leader</strong>, then giving the leader complete responsibility for managing the replicated log.
The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines.
A leader can fail or become disconnected from the other servers, in which case a new leader is elected.</p><p>Raft decomposes the consensus problem into three subproblems:</p><p><strong>Leader election</strong>: A new leader must be chosen when an existing leader fails.</p><p><strong>Log replication</strong>: The leader must accept log entries from clients and replicate them accross the cluster, forcing the other logs to agree with its own.</p><p><strong>Safety</strong>: If any server has applied a particular log entry to its state machine, then no other server may apply a different command for the same log index.</p><h2 id=raft-basics>Raft basics<a hidden class=anchor aria-hidden=true href=#raft-basics>#</a></h2><p>A raft cluster contains several servers, five is a typical number, which allows the system to tolerate two failures. At any given time each server is in one of three states: <strong>leader</strong>, <strong>follower</strong> or <strong>candidate</strong>.</p><p><strong>leader</strong>: In normal operation there is exactly one leader and all of the other servers are followers. The leader handles all client requests.</p><p><strong>follower</strong>: Followers simply respond to requests from leaders and candidates, if a client contacts a follower, the request is redirected to the leader.</p><p><strong>candidate</strong>: A candidate is a follower that wants to become a leader.</p><p>Raft divides time into <em>terms</em> of arbitraty length numbered with consecutive integers. Each term begins with an <em>election</em>, in which one or more candidates attempt to become leader. If a candidate wins the election, then it serves as leader for the rest of the term. If the election results in a split vote, the term ends with no leader and a new term with a new election begins shortly.</p><p>Each server stores a <em>current term</em> number, which increases monotonically over time. Current terms are exchanged whenever servers communicate and if one server&rsquo;s current term is smaller than the other&rsquo;s, then it updates its current term to the larger value. If a candidate or leader discovers that is term is out of date, it immediately reverts to follower state. If a server receives a request with a stale term number, it rejects the request.</p><h2 id=raft-servers-communication-with-remote-procedure-calls>Raft servers communication with remote procedure calls<a hidden class=anchor aria-hidden=true href=#raft-servers-communication-with-remote-procedure-calls>#</a></h2><p>Raft servers coomunicate using remote procedure calls. The basic consensus algorithm requires two types of RPCs:</p><p><strong>RequestVote</strong>: <strong>RequestVote</strong> RPCs are initiated by candidates during elections.</p><p><strong>AppendEntries</strong>: <strong>AppendEntries</strong> RPCs are initiated by leaders to replicate log entries and to provide a form of heartbeat.</p><p>Servers retry RPCs if they do not receive a response in a timely manner.</p><h2 id=leader-election>Leader election<a hidden class=anchor aria-hidden=true href=#leader-election>#</a></h2><p>Raft uses a heartbeat mechanism to trigger leader election. When servers start up, they begin as followers and remain in the follower state as long as it receives valid RPCs from a leader or candidate. Leaders send periodic heartbeats (<strong>AppendEntries</strong> RPCs that carry no log entries) to all followers in order to maintain their authority. If a follower receives no communication over a period of time called the <em>election timeout</em>, then it assumes there is no viable leader and begins an election to choose a new leader.</p><p>To begin an election, a follower increments its current term and transitions to candidate state. It then votes for itself and issues <strong>RequestVote</strong> RPCs in parallel to each server in the cluster. A candidate continues in the candidate state until one of three things happens:</p><p><strong>Wins election</strong>: The candidate wins the election and becomes the new leader.</p><p>A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term. Each server votes for at most one candidate in a given term, on a first-come-first-served basis.
Once a candidate wins an election, it becomes the leader and sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.</p><p><strong>Other server wins election</strong>: Another candidate wins the election and becomes the new leader.</p><p>During an election, a candidate may receive an <strong>AppendEntries</strong> RPC from another server claiming to be the leader. If the leader&rsquo;s term is at least as large as the candidate&rsquo;s current term, then the candidate recognizes the leader as legitimate and returns to follower state.
If the request term is smaller than the candidate&rsquo;s current term, the request is rejected as described in <a href=#Raft-basics>Raft basics</a>.</p><p><strong>Election timeout</strong>: A period of time goes by with no winner.</p><p>If many followers become candidates at the same time, votes could be split so that no candidate obtains a majority. When this happens, each candidate will time out and start a new election by increasing its term and iniating another round of <strong>RequestVote</strong> RPCs.</p><p>Elections timeouts are chosen randomly from the range like 150..300ms to ensure that split votes are rare and that hey are resolved quickly.</p><p>Each candidate gets a randomized election timeout at the start of an election, and it waits for the timeout to elapse before starting a new election.</p><h2 id=election-restriction>Election restriction<a hidden class=anchor aria-hidden=true href=#election-restriction>#</a></h2><p>Raft uses the voting process to prevent a candidate from winning an election unless its log contains all commited entries from previous terms. When a <strong>RequestVote</strong> RPC is made, the candidate includes the index and term of the last entry in its log, the server that receives the request (aka the voter) denies the request if its own log is more up-to-date than of the candidate. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.</p><h2 id=log-replication>Log replication<a hidden class=anchor aria-hidden=true href=#log-replication>#</a></h2><p>Once a leader has been elected, it begins servicing client requests that each contain a command to be executed by the replicated state machines. When a request is received, the leader appends the command to its log as a new entry, then issues <strong>AppendEntries</strong> RPCs in parallel to each of the servers to replicate the entry. Only after the entry has been replicated, the leader applies the entry to its state machine and returns the result of that execution to the client. The leader <strong>retries</strong> <strong>AppendEntries</strong> RPCs until all followers eventually store all log entries.</p><p>Logs are composed of sequentially numbered entries. Each entry contains the term in which it was created and a command for the state machine. The leader decides when it is safe to apply a log entry to the state machines, entries that have been applied to the state machines are called <em>committed</em>. Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines. A log entry is commited once the leader that created the entry has replicated it on a majority of the servers. The leader keeps track of the highest index it knows to be committed, and it includes that index in future <strong>AppendEntries</strong> RPCs so that other server eventually find out. Once a follower learns that a log entry is committed, it applies every entry up to the entry to its local state machine.</p><pre>
command  term
   │      │
   │      │   0    1    2    3    4    5    6    7      log index
   │      │ ┌────┬────┬────┬────┬────┬────┬────┬────┐
   │      ├─► 1  │ 1  │ 1  │ 2  │ 3  │ 3  │ 3  │ 3  │ │
   └──────┴─►x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │  leader
            └────┴────┴────┴────┴────┴────┴────┴────┘

            ┌────┬────┬────┬────┬────┐
            │ 1  │ 1  │ 1  │ 2  │ 3  │                │
            │x=3 │y=1 │y=9 │x=2 │x=0 │                │
            └────┴────┴────┴────┴────┘                │
                                                      │
            ┌────┬────┬────┬────┬────┬────┬────┬────┐ │
            │ 1  │ 1  │ 1  │ 2  │ 3  │ 3  │ 3  │ 3  │ │
            │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │x=4 │ │
            └────┴────┴────┴────┴────┴────┴────┴────┘ │
                                                      │  followers
            ┌────┬────┐                               │
            │ 1  │ 1  │                               │
            │x=3 │y=1 │                               │
            └────┴────┘                               │
                                                      │
            ┌────┬────┬────┬────┬────┬────┬────┐      │
            │ 1  │ 1  │ 1  │ 2  │ 3  │ 3  │ 3  │      │
            │x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │      │
            └────┴────┴────┴────┴────┴────┴────┘

            │                                  │
            └──────────────────────────────────┘
                      committed entries
</pre><p>Raft mantains the following properties:</p><ul><li><p>If two entries in different logs have the same index and term, then they store the same command.</p></li><li><p>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.</p></li></ul><h2 id=how-log-inconsistencies-are-handled>How log inconsistencies are handled<a hidden class=anchor aria-hidden=true href=#how-log-inconsistencies-are-handled>#</a></h2><p><strong>TLDR</strong>: Leader overwrites follower logs if they are out of sync.</p><p>In Raft, the leader handles inconsistencies by forcing the follower&rsquo;s logs to duplicate its own. This means that conflicting entries in follower logs will be overwritten with entries from the leader&rsquo;s log. To bring a follower&rsquo;s log into consistency with its own, the leader must find the latest log entry where the two logs agree, delete any entries in the follower&rsquo;s log after that point, and send the follower all of the leader&rsquo;s entries after that point. The leader maintains a <em>nextIndex</em> for each follower, which is the index of the next log the leader will send to that follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log.</p><h1 id=safety>Safety<a hidden class=anchor aria-hidden=true href=#safety>#</a></h1><p><strong>TLDR</strong>: Only servers that contain all of the entries commited in previous terms may become leader at any given term.</p><p>There&rsquo;s a problem with the Raft description so far:<br>For example, a follower might be unavailable while the leader commits several log entries, then it could be elected leader and overwrite these entries with new ones. As a result, different state machines might execute different command sequences. This is fixed by adding a restriction on which servers may be elected leader. The restriction ensures that the leader for any given term contains all of the entries commited in previous terms.</p><h2 id=restriction-on-committing-logs>Restriction on committing logs<a hidden class=anchor aria-hidden=true href=#restriction-on-committing-logs>#</a></h2><p>Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader&rsquo;s current term are commited by counting replicas.</p><h1 id=follower-and-candidate-crashes>Follower and candidate crashes<a hidden class=anchor aria-hidden=true href=#follower-and-candidate-crashes>#</a></h1><p>If a follower or candidate crashes, then future <strong>RequestVote</strong> and <strong>AppendEntries</strong> RPCs sent to it will fail. Raft handles these failures by retrying indefinitely, if the crashed server restarts, then the RPC will complete successfully. If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCS are idempotent, so this causes no harm. If a follower receives an <strong>AppendEntries</strong> request that includes log entries already present in its log, it ignores those entries in the new request.</p><h1 id=timing-and-availability>Timing and availability<a hidden class=anchor aria-hidden=true href=#timing-and-availability>#</a></h1><p>Raft will be able to elect and maintain a steady leader as long as the system satifies the following <em>timing requirement</em>:</p><p><em>broadcast_time &lt;= election_timeout &lt;= MTBF</em></p><p><strong>broad_cast_time</strong> is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses.</p><p><strong>election_timeout</strong> is the election timeout described in <a href=#Leader-election>Leader election</a>.</p><p><strong>MTBF</strong> is the average time between failures for a single server.</p><p>The broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbet messages required to keep followers from starting elections.</p><p>The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.</p><h1 id=cluster-membership-changes>Cluster membership changes<a hidden class=anchor aria-hidden=true href=#cluster-membership-changes>#</a></h1><p>Configuration changes are incorporated into the Raft consensus algorithm.
In Raft the cluster first switches to a transitional configuration called <em>joint consensus</em>, once the joint consensus has been committed, the system then transitions to the new configuration.<br>The join consensus combines both the old an new configurations:</p><ul><li>Log entries are replicated to all servers in both configurations.</li><li>Any server from either configuration may serve as leader.</li><li>Agreement for elections and entry commitment requires separate majorities from both the old and new configurations.</li></ul><p>The joint consensus allows individual servers to transition between configurations at different times without compromising safety.</p><h2 id=how-cluster-configurations-are-stored-and-communicated>How cluster configurations are stored and communicated<a hidden class=anchor aria-hidden=true href=#how-cluster-configurations-are-stored-and-communicated>#</a></h2><p>Cluster configurations are stored and communicated using special entries in the replicated log. When the leader receives a request to change the configuration from config_old (the current configuration) to config_new (the new configuration), it stores a tuple (config_old, config_new) as a log entry and replicates that entry. Once a given server adds the new configuration entry to its log, it uses that configuration for all future decisions even if the entry has not been committed yet.</p><h1 id=log-compaction>Log compaction<a hidden class=anchor aria-hidden=true href=#log-compaction>#</a></h1><p><strong>TLDR</strong>: We don&rsquo;t have infinite memory, discard logs that aren&rsquo;t needed anymore.</p><p>Raft&rsquo;s log grows during normal operation to incorporate more client requests, but in a practical system, it cannot grow without bound. As the log grows longer, it occupies more spaces and takes more time to replay.</p><p>Snapshotting is the simplest approach to compaction. In snapshotting the entire current system state is written to a <em>snapshot</em> on stable storage, then the tire log up to that point is discarded. Snapshotting is used in Chubby and ZooKeeper and in Raft as well.</p><h2 id=the-basic-idea-of-snapshotting-in-raft>The basic idea of snapshotting in Raft:<a hidden class=anchor aria-hidden=true href=#the-basic-idea-of-snapshotting-in-raft>#</a></h2><p><strong>TLDR</strong>: Add the current machine state to the log and delete all logs used to get to this state. The snapshot should also be written to stable storage.</p><p>Each server takes snapshots independently, covering just the commited entries in its log. Most of the work consists of the state machine writing its current state to the snapshot. Raft also includes a small amount of metadata in the snapshot: the <em>last included index</em> which is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied), and the <em>last included term</em> which is the term of the entry. The snapshot also includes the latest configuration. Once a server completes a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.</p><pre>
  0    1    2    3    4    5    6     log index
┌────┬────┬────┬────┬────┬────┬────┐
│ 1  │ 1  │ 1  │ 2  │ 3  │ 3  │ 3  │  │ before snapshot
│x=3 │y=1 │y=9 │x=2 │x=0 │y=7 │x=5 │  │
└────┴────┴────┴────┴────┴────┴────┘


         snapshot
┌────────────────────────┬────┬────┐
│last included index: 4  │ 3  │ 3  │  │
│last included term: 3   │y=7 │x=5 │  │
│state machine state:    ├────┴────┘  │ after snapshot
│ x = 0                  │            │
│ y = 9                  │            │
└────────────────────────┘

│                        │
└────────────────────────┘
    committed entries
</pre><h2 id=when-a-new-follower-joins-the-cluster>When a new follower joins the cluster<a hidden class=anchor aria-hidden=true href=#when-a-new-follower-joins-the-cluster>#</a></h2><p>The way to bring a new foller up-to-date is for the leader to send it a snapshot over the network. The leader uses a new RPC called <strong>InstallSnapShot</strong> to send snapshots to followers that are too far behind because they are new or because they are too slow.</p><p>When a follower receives a snapshot with this RPC:</p><p>If the snapshot contains new information on in the follower&rsquo;s log, the follower discards and replaces its log with the snapshot.</p><p>If the snapshot contains only a prefix of its log, then log entries covered by the snapshot are deleted bu entries following the snapshot are still valid and must be retained.</p><h1 id=client-interaction>Client interaction<a hidden class=anchor aria-hidden=true href=#client-interaction>#</a></h1><p>Clients of Raft send all of their requests to the leader.</p><h2 id=how-clients-find-the-leader>How clients find the leader<a hidden class=anchor aria-hidden=true href=#how-clients-find-the-leader>#</a></h2><p>When a client starts up, it connects to a randomly-chosen server. If the client&rsquo;s choice is not the leader, that server will reject the client&rsquo;s request and supply information about the most recent leader it has heard from.</p><h2 id=command-deduplication>Command deduplication<a hidden class=anchor aria-hidden=true href=#command-deduplication>#</a></h2><p>For example, if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing to be executed a second time.</p><p>The solution is for clients to assign unique serial numbers to every command. Then, the state machine tracks the latest serial number processed for each client, along with the associated response. If it receives a command whose serial number ha already been executed, it responses immediatelly without re-executing the request.</p><h2 id=read-only-operations-should-not-return-stale-data>Read-only operations should not return stale data<a hidden class=anchor aria-hidden=true href=#read-only-operations-should-not-return-stale-data>#</a></h2><p>When a leader responds to a request, its log could be outdated with a new leader had been elected in the meantime.</p><p>To avoid this situation:</p><p>A leader must have the latest information on which entries are committed. Because of that, each leader commits a blank <em>no-op</em> entry into the log at the start of its term.</p><p>A leader must check whether it has been deposed before processing a read-only request. Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only request.</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><p>In Search of an Understandable Consensus Algorithm
(Extended Version) - <a href=https://raft.github.io/raft.pdf>https://raft.github.io/raft.pdf</a><br>Designing for Understandability: The Raft Consensus Algorithm - <a href="https://www.youtube.com/watch?v=vYp4LYbnnW8">https://www.youtube.com/watch?v=vYp4LYbnnW8</a>
&ldquo;Raft - The Understandable Distributed Protocol&rdquo; by Ben Johnson (2013) - <a href="https://www.youtube.com/watch?v=ro2fU8_mr2w">https://www.youtube.com/watch?v=ro2fU8_mr2w</a><br><a href=https://github.com/hashicorp/raft>https://github.com/hashicorp/raft</a><br>MIT 6.824: Distributed Systems (Spring 2020) Lecture 6: Fault Tolerance: Raft - <a href="https://www.youtube.com/watch?v=64Zp3tzNbpE&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB&index=6">https://www.youtube.com/watch?v=64Zp3tzNbpE&list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB&index=6</a></p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://poorlydefinedbehaviour.github.io/posts/swim_protocol/><span class=title>« Prev Page</span><br><span>Swim protocol</span></a>
<a class=next href=https://poorlydefinedbehaviour.github.io/posts/bloom_filter/><span class=title>Next Page »</span><br><span>Bloom filter</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Notes taken from the Raft paper on twitter" href="https://twitter.com/intent/tweet/?text=Notes%20taken%20from%20the%20Raft%20paper&url=https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes taken from the Raft paper on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f&title=Notes%20taken%20from%20the%20Raft%20paper&summary=Notes%20taken%20from%20the%20Raft%20paper&source=https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes taken from the Raft paper on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f&title=Notes%20taken%20from%20the%20Raft%20paper"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes taken from the Raft paper on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes taken from the Raft paper on whatsapp" href="https://api.whatsapp.com/send?text=Notes%20taken%20from%20the%20Raft%20paper%20-%20https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Notes taken from the Raft paper on telegram" href="https://telegram.me/share/url?text=Notes%20taken%20from%20the%20Raft%20paper&url=https%3a%2f%2fpoorlydefinedbehaviour.github.io%2fposts%2fraft_notes%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://poorlydefinedbehaviour.github.io></a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>